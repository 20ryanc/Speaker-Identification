{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task description\n- Classify the speakers of given features.\n- Main goal: Learn how to use transformer.\n- Baselines:\n  - Easy: Run sample code and know how to use transformer.\n  - Medium: Know how to adjust parameters of transformer.\n  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n\n- Other links\n  - Kaggle: [link](https://www.kaggle.com/t/ac77388c90204a4c8daebeddd40ff916)\n  - Slide: [link](https://docs.google.com/presentation/d/1HLAj7UUIjZOycDe7DaVLSwJfXVd3bXPOyzSb6Zk3hYU/edit?usp=sharing)\n  - Data: [link](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)\n\n# Download dataset\n- Data is [here](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)","metadata":{"id":"C_jdZ5vHJ4A9"}},{"cell_type":"markdown","source":"## Fix Random Seed","metadata":{"id":"ENWVAUDVJtVY"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_seed(87)","metadata":{"id":"E6burzCXIyuA","execution":{"iopub.status.busy":"2023-05-02T14:34:42.353056Z","iopub.execute_input":"2023-05-02T14:34:42.353361Z","iopub.status.idle":"2023-05-02T14:34:45.230082Z","shell.execute_reply.started":"2023-05-02T14:34:42.353333Z","shell.execute_reply":"2023-05-02T14:34:45.228603Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\n## Dataset\n- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n- We randomly select 600 speakers from Voxceleb2.\n- Then preprocess the raw waveforms into mel-spectrograms.\n\n- Args:\n  - data_dir: The path to the data directory.\n  - metadata_path: The path to the metadata.\n  - segment_len: The length of audio segment for training. \n- The architecture of data directory \\\\\n  - data directory \\\\\n  |---- metadata.json \\\\\n  |---- testdata.json \\\\\n  |---- mapping.json \\\\\n  |---- uttr-{random string}.pt \\\\\n\n- The information in metadata\n  - \"n_mels\": The dimention of mel-spectrogram.\n  - \"speakers\": A dictionary. \n    - Key: speaker ids.\n    - value: \"feature_path\" and \"mel_len\"\n\n\nFor efficiency, we segment the mel-spectrograms into segments in the traing step.","metadata":{"id":"k7dVbxW2LASN"}},{"cell_type":"code","source":"!pip install conformer","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:34:45.236327Z","iopub.execute_input":"2023-05-02T14:34:45.236962Z","iopub.status.idle":"2023-05-02T14:34:57.273193Z","shell.execute_reply.started":"2023-05-02T14:34:45.236921Z","shell.execute_reply":"2023-05-02T14:34:57.271949Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting conformer\n  Downloading conformer-0.2.5-py3-none-any.whl (4.1 kB)\nCollecting einops\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from conformer) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->conformer) (4.4.0)\nInstalling collected packages: einops, conformer\nSuccessfully installed conformer-0.2.5 einops-0.6.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport random\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom conformer import ConformerBlock\n \nclass myDataset(Dataset):\n    def __init__(self, data_dir, segment_len=128):\n        self.data_dir = data_dir\n        self.segment_len = segment_len\n\n        # Load the mapping from speaker neme to their corresponding id. \n        mapping_path = Path(data_dir) / \"mapping.json\"\n        mapping = json.load(mapping_path.open())\n        self.speaker2id = mapping[\"speaker2id\"]\n\n        # Load metadata of training data.\n        metadata_path = Path(data_dir) / \"metadata.json\"\n        metadata = json.load(open(metadata_path))[\"speakers\"]\n\n        # Get the total number of speaker.\n        self.speaker_num = len(metadata.keys())\n        self.data = []\n        for speaker in metadata.keys():\n            for utterances in metadata[speaker]:\n                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n\n    def __len__(self):\n            return len(self.data)\n\n    def __getitem__(self, index):\n        feat_path, speaker = self.data[index]\n        # Load preprocessed mel-spectrogram.\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n\n        # Segmemt mel-spectrogram into \"segment_len\" frames.\n        if len(mel) > self.segment_len:\n            # Randomly get the starting point of the segment.\n            start = random.randint(0, len(mel) - self.segment_len)\n            # Get a segment with \"segment_len\" frames.\n            mel = torch.FloatTensor(mel[start:start+self.segment_len])\n        else:\n            mel = torch.FloatTensor(mel)\n        # Turn the speaker id into long for computing loss later.\n        speaker = torch.FloatTensor([speaker]).long()\n        return mel, speaker\n \n    def get_speaker_number(self):\n        return self.speaker_num","metadata":{"id":"KpuGxl4CI2pr","execution":{"iopub.status.busy":"2023-05-02T14:34:57.276479Z","iopub.execute_input":"2023-05-02T14:34:57.276957Z","iopub.status.idle":"2023-05-02T14:34:57.299547Z","shell.execute_reply.started":"2023-05-02T14:34:57.276912Z","shell.execute_reply":"2023-05-02T14:34:57.298259Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader\n- Split dataset into training dataset(90%) and validation dataset(10%).\n- Create dataloader to iterate the data.","metadata":{"id":"668hverTMlGN"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef collate_batch(batch):\n    # Process features within a batch.\n    \"\"\"Collate a batch of data.\"\"\"\n    mel, speaker = zip(*batch)\n    # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n    mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n    # mel: (batch size, length, 40)\n    return mel, torch.FloatTensor(speaker).long()\n\n\ndef get_dataloader(data_dir, batch_size, n_workers):\n    \"\"\"Generate dataloader\"\"\"\n    dataset = myDataset(data_dir)\n    speaker_num = dataset.get_speaker_number()\n    # Split dataset into training dataset and validation dataset\n    trainlen = int(0.9 * len(dataset))\n    lengths = [trainlen, len(dataset) - trainlen]\n    trainset, validset = random_split(dataset, lengths)\n\n    train_loader = DataLoader(\n        trainset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=n_workers,\n        pin_memory=True,\n        collate_fn=collate_batch,\n    )\n    valid_loader = DataLoader(\n        validset,\n        batch_size=batch_size,\n        num_workers=n_workers,\n        drop_last=True,\n        pin_memory=True,\n        collate_fn=collate_batch,\n    )\n\n    return train_loader, valid_loader, speaker_num","metadata":{"id":"B7c2gZYoJDRS","execution":{"iopub.status.busy":"2023-05-02T14:34:57.303298Z","iopub.execute_input":"2023-05-02T14:34:57.303872Z","iopub.status.idle":"2023-05-02T14:34:57.313765Z","shell.execute_reply.started":"2023-05-02T14:34:57.303842Z","shell.execute_reply":"2023-05-02T14:34:57.312552Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- TransformerEncoderLayer:\n  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n  - Parameters:\n    - d_model: the number of expected features of the input (required).\n\n    - nhead: the number of heads of the multiheadattention models (required).\n\n    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n\n    - dropout: the dropout value (default=0.1).\n\n    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n\n- TransformerEncoder:\n  - TransformerEncoder is a stack of N transformer encoder layers\n  - Parameters:\n    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n\n    - num_layers: the number of sub-encoder-layers in the encoder (required).\n\n    - norm: the layer normalization component (optional).","metadata":{"id":"5FOSZYxrMqhc"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn import CrossEntropyLoss","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:34:57.315809Z","iopub.execute_input":"2023-05-02T14:34:57.316344Z","iopub.status.idle":"2023-05-02T14:34:57.326830Z","shell.execute_reply.started":"2023-05-02T14:34:57.316301Z","shell.execute_reply":"2023-05-02T14:34:57.325824Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class SelfAttentionPooling(nn.Module):\n    def __init__(self, input_dim):\n        super(SelfAttentionPooling, self).__init__()\n        self.W = nn.Linear(input_dim, 1)\n        self.softmax = nn.Softmax(dim=1)\n    \n    def forward(self, batch_rep):\n        \"\"\"\n        input:\n            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n      \n        attention_weight:\n            att_w : size (N, T, 1)\n    \n        return:\n            utter_rep: size (N, H)\n        \"\"\"\n        out = self.W(batch_rep).squeeze(-1)\n        att_w = self.softmax(out).unsqueeze(-1)\n        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n\n        return utter_rep","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:34:57.328999Z","iopub.execute_input":"2023-05-02T14:34:57.330225Z","iopub.status.idle":"2023-05-02T14:34:57.338532Z","shell.execute_reply.started":"2023-05-02T14:34:57.330185Z","shell.execute_reply":"2023-05-02T14:34:57.337557Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Transformer Model\nclass TransformerClassifier(nn.Module):\n    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n        super().__init__()\n        # Project the dimension of features from that of input into d_model.\n        self.prenet = nn.Linear(40, d_model)\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, dim_feedforward=256, nhead=2\n        )\n        # self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n\n        # Project the the dimension of features from d_model into speaker nums.\n        self.pred_layer = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, n_spks),\n        )\n\n    def forward(self, mels):\n        \"\"\"\n        args:\n            mels: (batch size, length, 40)\n        return:\n            out: (batch size, n_spks)\n        \"\"\"\n        out = self.prenet(mels)\n        out = out.permute(1, 0, 2)\n        out = self.encoder_layer(out)\n        out = out.transpose(0, 1)\n        stats = out.mean(dim=1)\n\n        out = self.pred_layer(stats)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:34:57.341392Z","iopub.execute_input":"2023-05-02T14:34:57.342074Z","iopub.status.idle":"2023-05-02T14:34:57.351559Z","shell.execute_reply.started":"2023-05-02T14:34:57.342037Z","shell.execute_reply":"2023-05-02T14:34:57.350565Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Transformer Model\nclass TransformerClassifierPooling(nn.Module):\n    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n        super().__init__()\n        # Project the dimension of features from that of input into d_model.\n        self.prenet = nn.Linear(40, d_model)\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, dim_feedforward=256, nhead=2\n        )\n        # self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n\n        # Project the the dimension of features from d_model into speaker nums.\n\n        self.attention_pooling = SelfAttentionPooling(d_model)\n        self.linear_model = nn.Linear(d_model, n_spks)\n\n    def forward(self, mels):\n        \"\"\"\n        args:\n            mels: (batch size, length, 40)\n        return:\n            out: (batch size, n_spks)\n        \"\"\"\n        out = self.prenet(mels)\n        out = out.permute(1, 0, 2)\n        out = self.encoder_layer(out)\n        out = out.transpose(0, 1)\n\n        out = self.attention_pooling(out)\n        out = self.linear_model(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:34:57.353219Z","iopub.execute_input":"2023-05-02T14:34:57.353621Z","iopub.status.idle":"2023-05-02T14:34:57.363069Z","shell.execute_reply.started":"2023-05-02T14:34:57.353583Z","shell.execute_reply":"2023-05-02T14:34:57.362052Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Confomer Model \nclass ConformerClassifier(nn.Module):\n    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n        super().__init__()\n        # Project the dimension of features from that of input into d_model.\n        self.prenet = nn.Linear(40, d_model)\n        \n        self.block1 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.,\n                    ff_dropout = 0.,\n                    conv_dropout = 0.\n                    )\n        \n        self.block2 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.,\n                    ff_dropout = 0.,\n                    conv_dropout = 0.\n                    )\n        \n        self.block3 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.1,\n                    ff_dropout = 0.1,\n                    conv_dropout = 0.1\n                    )\n        \n        # Project the the dimension of features from d_model into speaker nums.\n        self.pred_layer = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, n_spks),\n        )\n\n    def forward(self, mels):\n        \"\"\"\n        args:\n            mels: (batch size, length, 40)\n        return:\n            out: (batch size, n_spks)\n        \"\"\"\n        # out: (batch size, length, d_model)\n        out = self.prenet(mels)\n        # The encoder layer expect features in the shape of (length, batch size, d_model).\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        # out: (batch, n_spks)\n        out = out.mean(dim=1)\n        out = self.pred_layer(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:34:57.364893Z","iopub.execute_input":"2023-05-02T14:34:57.365916Z","iopub.status.idle":"2023-05-02T14:34:57.379147Z","shell.execute_reply.started":"2023-05-02T14:34:57.365885Z","shell.execute_reply":"2023-05-02T14:34:57.378191Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Confomer Model \nclass ConformerClassifierPooling(nn.Module):\n    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n        super().__init__()\n        # Project the dimension of features from that of input into d_model.\n        self.prenet = nn.Linear(40, d_model)\n        \n        self.block1 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.,\n                    ff_dropout = 0.,\n                    conv_dropout = 0.\n                    )\n        \n        self.block2 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.,\n                    ff_dropout = 0.,\n                    conv_dropout = 0.\n                    )\n        \n        self.block3 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.1,\n                    ff_dropout = 0.1,\n                    conv_dropout = 0.1\n                    )\n        \n        # Project the the dimension of features from d_model into speaker nums.\n        self.attention_pooling = SelfAttentionPooling(d_model)\n        self.linear_model = nn.Linear(d_model, n_spks)\n\n    def forward(self, mels):\n        \"\"\"\n        args:\n            mels: (batch size, length, 40)\n        return:\n            out: (batch size, n_spks)\n        \"\"\"\n        # out: (batch size, length, d_model)\n        out = self.prenet(mels)\n        # The encoder layer expect features in the shape of (length, batch size, d_model).\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        # out: (batch, n_spks)\n        out = self.attention_pooling(out)\n        out = self.linear_model(out)\n        return out","metadata":{"id":"iXZ5B0EKJGs8","execution":{"iopub.status.busy":"2023-05-02T14:34:57.383850Z","iopub.execute_input":"2023-05-02T14:34:57.384148Z","iopub.status.idle":"2023-05-02T14:34:57.394803Z","shell.execute_reply.started":"2023-05-02T14:34:57.384104Z","shell.execute_reply":"2023-05-02T14:34:57.394075Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Learning rate schedule\n- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n- The warmup schedule\n  - Set learning rate to 0 in the beginning.\n  - The learning rate increases linearly from 0 to initial learning rate during warmup period.","metadata":{"id":"W7yX8JinM5Ly"}},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef get_cosine_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n    ):\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n        The optimizer for which to schedule the learning rate.\n        num_warmup_steps (:obj:`int`):\n        The number of steps for the warmup phase.\n        num_training_steps (:obj:`int`):\n        The total number of training steps.\n        num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n        The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n        following a half-cosine).\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n        The index of the last epoch when resuming training.\n\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    def lr_lambda(current_step):\n        # Warmup\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        # decadence\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps)\n        )\n        return max(\n            0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n        )\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"id":"ykt0N1nVJJi2","execution":{"iopub.status.busy":"2023-05-02T14:34:57.396049Z","iopub.execute_input":"2023-05-02T14:34:57.397069Z","iopub.status.idle":"2023-05-02T14:34:57.407643Z","shell.execute_reply.started":"2023-05-02T14:34:57.397031Z","shell.execute_reply":"2023-05-02T14:34:57.406478Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Model Function\n- Model forward function.","metadata":{"id":"-LN2XkteM_uH"}},{"cell_type":"code","source":"import torch\n\n\ndef model_fn(batch, model, criterion, device, isAM = False):\n    \"\"\"Forward a batch through the model.\"\"\"\n\n    mels, labels = batch\n    mels = mels.to(device)\n    labels = labels.to(device)\n\n    outs = model(mels)\n    if isAM:\n        loss = criterion(outs, labels, scale=30.0, margin=0.4)\n        # Get the speaker id with highest probability.\n        preds = outs.argmax(1)\n        # Compute accuracy.\n        accuracy = torch.mean((preds == labels).float())\n        \n        return loss, accuracy\n    \n    loss = criterion(outs, labels)\n    # Get the speaker id with highest probability.\n    preds = outs.argmax(1)\n    # Compute accuracy.\n    accuracy = torch.mean((preds == labels).float())\n\n    return loss, accuracy","metadata":{"id":"N-rr8529JMz0","execution":{"iopub.status.busy":"2023-05-02T14:34:57.409100Z","iopub.execute_input":"2023-05-02T14:34:57.409960Z","iopub.status.idle":"2023-05-02T14:34:57.419915Z","shell.execute_reply.started":"2023-05-02T14:34:57.409912Z","shell.execute_reply":"2023-05-02T14:34:57.419185Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Validate\n- Calculate accuracy of the validation set.","metadata":{"id":"cwM_xyOtNCI2"}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, random_split\n\n\ndef valid(dataloader, model, criterion, device, isAM = False): \n    \"\"\"Validate on validation set.\"\"\"\n\n    model.eval()\n    running_loss = 0.0\n    running_accuracy = 0.0\n    pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n    \n    for i, batch in enumerate(dataloader):\n        with torch.no_grad():\n            loss, accuracy = model_fn(batch, model, criterion, device, isAM)\n            running_loss += loss.item()\n            running_accuracy += accuracy.item()\n\n        pbar.update(dataloader.batch_size)\n        pbar.set_postfix(\n            loss=f\"{running_loss / (i+1):.2f}\",\n            accuracy=f\"{running_accuracy / (i+1):.2f}\",\n        )\n\n    pbar.close()\n    model.train()\n\n    return running_accuracy / len(dataloader)","metadata":{"id":"YAiv6kpdJRTJ","execution":{"iopub.status.busy":"2023-05-02T14:34:57.421094Z","iopub.execute_input":"2023-05-02T14:34:57.421823Z","iopub.status.idle":"2023-05-02T14:34:57.431848Z","shell.execute_reply.started":"2023-05-02T14:34:57.421776Z","shell.execute_reply":"2023-05-02T14:34:57.431122Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class AMSoftmax(nn.Module):\n    def __init__(self):\n        super(AMSoftmax, self).__init__()\n\n    def forward(self, input, target, scale=10.0, margin=0.35):\n        # self.it += 1\n        cos_theta = input\n        target = target.view(-1, 1)  # size=(B,1)\n\n        index = cos_theta.data * 0.0  # size=(B,Classnum)\n        index.scatter_(1, target.data.view(-1, 1), 1)\n        index = index.byte()\n        index = Variable(index)\n\n        output = cos_theta * 1.0  # size=(B,Classnum)\n        output[index] -= margin\n        output = output * scale\n\n        logpt = F.log_softmax(output)\n        logpt = logpt.gather(1, target)\n        logpt = logpt.view(-1)\n\n        loss = -1 * logpt\n        loss = loss.mean()\n\n        return loss\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:34:57.433229Z","iopub.execute_input":"2023-05-02T14:34:57.433941Z","iopub.status.idle":"2023-05-02T14:34:57.442578Z","shell.execute_reply.started":"2023-05-02T14:34:57.433904Z","shell.execute_reply":"2023-05-02T14:34:57.441842Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Model Train","metadata":{"id":"g6ne9G-eNEdG"}},{"cell_type":"code","source":"def parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"/kaggle/input/cs4650dataset/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 100000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    transformer_model = TransformerClassifier(n_spks=speaker_num).to(device)\n    criterion = CrossEntropyLoss()\n    optimizer = AdamW(transformer_model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, transformer_model, criterion, device)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata transformer model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, transformer_model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = transformer_model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:34:57.444098Z","iopub.execute_input":"2023-05-02T14:34:57.444736Z","iopub.status.idle":"2023-05-02T15:13:42.333959Z","shell.execute_reply.started":"2023-05-02T14:34:57.444699Z","shell.execute_reply":"2023-05-02T15:13:42.332300Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"},{"name":"stdout","text":"[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:27<00:00, 22.90 step/s, accuracy=0.09, loss=3.96, step=2000]\nValid: 100% 5664/5667 [00:08<00:00, 684.70 uttr/s, accuracy=0.16, loss=4.09]\nTrain: 100% 2000/2000 [00:43<00:00, 46.42 step/s, accuracy=0.16, loss=3.70, step=4000]\nValid: 100% 5664/5667 [00:03<00:00, 1884.13 uttr/s, accuracy=0.25, loss=3.52]\nTrain: 100% 2000/2000 [00:40<00:00, 48.96 step/s, accuracy=0.41, loss=2.75, step=6000]\nValid: 100% 5664/5667 [00:03<00:00, 1833.39 uttr/s, accuracy=0.35, loss=2.99]\nTrain: 100% 2000/2000 [00:41<00:00, 48.56 step/s, accuracy=0.41, loss=2.65, step=8000]\nValid: 100% 5664/5667 [00:03<00:00, 1677.82 uttr/s, accuracy=0.38, loss=2.79]\nTrain: 100% 2000/2000 [00:41<00:00, 47.94 step/s, accuracy=0.28, loss=3.15, step=1e+4]\nValid: 100% 5664/5667 [00:03<00:00, 1773.72 uttr/s, accuracy=0.42, loss=2.62]\nTrain:   0% 10/2000 [00:00<00:41, 48.46 step/s, accuracy=0.47, loss=2.52, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.4160)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:43<00:00, 45.85 step/s, accuracy=0.47, loss=2.77, step=12000]\nValid: 100% 5664/5667 [00:03<00:00, 1860.83 uttr/s, accuracy=0.46, loss=2.42]\nTrain: 100% 2000/2000 [00:44<00:00, 44.46 step/s, accuracy=0.59, loss=1.49, step=14000]\nValid: 100% 5664/5667 [00:03<00:00, 1842.43 uttr/s, accuracy=0.46, loss=2.43]\nTrain: 100% 2000/2000 [00:41<00:00, 48.53 step/s, accuracy=0.47, loss=2.69, step=16000]\nValid: 100% 5664/5667 [00:03<00:00, 1841.62 uttr/s, accuracy=0.50, loss=2.26]\nTrain: 100% 2000/2000 [00:41<00:00, 48.15 step/s, accuracy=0.62, loss=1.80, step=18000]\nValid: 100% 5664/5667 [00:03<00:00, 1739.95 uttr/s, accuracy=0.48, loss=2.28]\nTrain: 100% 2000/2000 [00:43<00:00, 45.65 step/s, accuracy=0.44, loss=2.06, step=2e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1791.24 uttr/s, accuracy=0.53, loss=2.13]\nTrain:   0% 10/2000 [00:00<00:37, 53.57 step/s, accuracy=0.75, loss=1.48, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.5305)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:42<00:00, 46.60 step/s, accuracy=0.56, loss=1.85, step=22000]\nValid: 100% 5664/5667 [00:03<00:00, 1539.11 uttr/s, accuracy=0.52, loss=2.07]\nTrain: 100% 2000/2000 [00:44<00:00, 45.26 step/s, accuracy=0.56, loss=1.83, step=24000]\nValid: 100% 5664/5667 [00:03<00:00, 1690.66 uttr/s, accuracy=0.55, loss=2.01]\nTrain: 100% 2000/2000 [00:45<00:00, 43.48 step/s, accuracy=0.62, loss=1.33, step=26000]\nValid: 100% 5664/5667 [00:03<00:00, 1517.66 uttr/s, accuracy=0.56, loss=1.94]\nTrain: 100% 2000/2000 [00:43<00:00, 45.95 step/s, accuracy=0.59, loss=1.48, step=28000]\nValid: 100% 5664/5667 [00:03<00:00, 1827.98 uttr/s, accuracy=0.55, loss=1.99]\nTrain: 100% 2000/2000 [00:41<00:00, 48.71 step/s, accuracy=0.59, loss=1.95, step=3e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1855.54 uttr/s, accuracy=0.56, loss=1.90]\nTrain:   0% 10/2000 [00:00<00:34, 56.89 step/s, accuracy=0.59, loss=1.57, step=3e+4]","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.5629)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:41<00:00, 48.16 step/s, accuracy=0.72, loss=1.06, step=32000]\nValid: 100% 5664/5667 [00:03<00:00, 1806.12 uttr/s, accuracy=0.58, loss=1.83]\nTrain: 100% 2000/2000 [00:40<00:00, 49.10 step/s, accuracy=0.50, loss=2.17, step=34000]\nValid: 100% 5664/5667 [00:03<00:00, 1854.81 uttr/s, accuracy=0.58, loss=1.85]\nTrain: 100% 2000/2000 [00:45<00:00, 43.86 step/s, accuracy=0.59, loss=1.48, step=36000]\nValid: 100% 5664/5667 [00:03<00:00, 1777.96 uttr/s, accuracy=0.58, loss=1.83]\nTrain: 100% 2000/2000 [00:41<00:00, 48.73 step/s, accuracy=0.66, loss=1.23, step=38000]\nValid: 100% 5664/5667 [00:03<00:00, 1863.75 uttr/s, accuracy=0.60, loss=1.78]\nTrain: 100% 2000/2000 [00:46<00:00, 42.89 step/s, accuracy=0.62, loss=1.60, step=4e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1828.36 uttr/s, accuracy=0.60, loss=1.73]\nTrain:   1% 11/2000 [00:00<00:30, 64.67 step/s, accuracy=0.69, loss=1.05, step=4e+4]","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.6033)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:46<00:00, 42.77 step/s, accuracy=0.75, loss=0.99, step=42000]\nValid: 100% 5664/5667 [00:04<00:00, 1188.33 uttr/s, accuracy=0.62, loss=1.69]\nTrain: 100% 2000/2000 [00:41<00:00, 48.69 step/s, accuracy=0.72, loss=0.99, step=44000]\nValid: 100% 5664/5667 [00:03<00:00, 1776.19 uttr/s, accuracy=0.61, loss=1.68]\nTrain: 100% 2000/2000 [00:41<00:00, 48.19 step/s, accuracy=0.59, loss=1.55, step=46000]\nValid: 100% 5664/5667 [00:03<00:00, 1723.25 uttr/s, accuracy=0.62, loss=1.66]\nTrain: 100% 2000/2000 [00:42<00:00, 47.41 step/s, accuracy=0.62, loss=1.25, step=48000]\nValid: 100% 5664/5667 [00:03<00:00, 1517.47 uttr/s, accuracy=0.62, loss=1.66]\nTrain: 100% 2000/2000 [00:41<00:00, 48.32 step/s, accuracy=0.72, loss=1.10, step=5e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1801.00 uttr/s, accuracy=0.64, loss=1.57]\nTrain:   0% 10/2000 [00:00<00:38, 51.45 step/s, accuracy=0.69, loss=1.07, step=5e+4]","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.6402)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:40<00:00, 49.04 step/s, accuracy=0.75, loss=1.11, step=52000]\nValid: 100% 5664/5667 [00:03<00:00, 1837.18 uttr/s, accuracy=0.64, loss=1.60]\nTrain: 100% 2000/2000 [00:41<00:00, 48.42 step/s, accuracy=0.59, loss=1.61, step=54000]\nValid: 100% 5664/5667 [00:03<00:00, 1849.01 uttr/s, accuracy=0.64, loss=1.58]\nTrain: 100% 2000/2000 [00:41<00:00, 48.35 step/s, accuracy=0.72, loss=1.18, step=56000]\nValid: 100% 5664/5667 [00:03<00:00, 1868.41 uttr/s, accuracy=0.65, loss=1.51]\nTrain: 100% 2000/2000 [00:40<00:00, 49.15 step/s, accuracy=0.69, loss=1.32, step=58000]\nValid: 100% 5664/5667 [00:03<00:00, 1651.23 uttr/s, accuracy=0.66, loss=1.51]\nTrain: 100% 2000/2000 [00:40<00:00, 48.81 step/s, accuracy=0.72, loss=1.46, step=6e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1870.66 uttr/s, accuracy=0.66, loss=1.52]\nTrain:   0% 10/2000 [00:00<00:33, 59.44 step/s, accuracy=0.88, loss=0.52, step=6e+4]","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.6566)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:41<00:00, 48.58 step/s, accuracy=0.84, loss=0.63, step=62000]\nValid: 100% 5664/5667 [00:03<00:00, 1865.80 uttr/s, accuracy=0.67, loss=1.45]\nTrain: 100% 2000/2000 [00:41<00:00, 48.29 step/s, accuracy=0.91, loss=0.57, step=64000]\nValid: 100% 5664/5667 [00:03<00:00, 1806.23 uttr/s, accuracy=0.67, loss=1.45]\nTrain: 100% 2000/2000 [00:40<00:00, 49.15 step/s, accuracy=0.78, loss=0.83, step=66000]\nValid: 100% 5664/5667 [00:03<00:00, 1839.91 uttr/s, accuracy=0.67, loss=1.47]\nTrain: 100% 2000/2000 [00:41<00:00, 48.69 step/s, accuracy=0.81, loss=0.68, step=68000]\nValid: 100% 5664/5667 [00:03<00:00, 1842.14 uttr/s, accuracy=0.67, loss=1.43]\nTrain: 100% 2000/2000 [00:41<00:00, 47.79 step/s, accuracy=0.75, loss=1.26, step=7e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1812.85 uttr/s, accuracy=0.67, loss=1.44]\nTrain:   0% 10/2000 [00:00<00:34, 58.44 step/s, accuracy=0.75, loss=1.03, step=7e+4]","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.6728)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:42<00:00, 47.02 step/s, accuracy=0.69, loss=1.58, step=72000]\nValid: 100% 5664/5667 [00:03<00:00, 1731.02 uttr/s, accuracy=0.69, loss=1.36]\nTrain: 100% 2000/2000 [00:41<00:00, 48.06 step/s, accuracy=0.75, loss=0.94, step=74000]\nValid: 100% 5664/5667 [00:03<00:00, 1839.14 uttr/s, accuracy=0.69, loss=1.36]\nTrain: 100% 2000/2000 [00:40<00:00, 49.20 step/s, accuracy=0.84, loss=0.50, step=76000]\nValid: 100% 5664/5667 [00:03<00:00, 1844.04 uttr/s, accuracy=0.69, loss=1.36]\nTrain: 100% 2000/2000 [00:42<00:00, 46.92 step/s, accuracy=0.81, loss=0.82, step=78000]\nValid: 100% 5664/5667 [00:03<00:00, 1797.12 uttr/s, accuracy=0.70, loss=1.34]\nTrain: 100% 2000/2000 [00:41<00:00, 47.77 step/s, accuracy=0.78, loss=0.93, step=8e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1740.97 uttr/s, accuracy=0.71, loss=1.34]\nTrain:   0% 9/2000 [00:00<00:49, 39.92 step/s, accuracy=0.75, loss=0.87, step=8e+4]","output_type":"stream"},{"name":"stdout","text":"Step 80000, best model saved. (accuracy=0.7067)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:40<00:00, 49.48 step/s, accuracy=0.78, loss=0.65, step=82000]\nValid: 100% 5664/5667 [00:03<00:00, 1779.94 uttr/s, accuracy=0.70, loss=1.34]\nTrain: 100% 2000/2000 [00:41<00:00, 47.96 step/s, accuracy=0.78, loss=0.96, step=84000]\nValid: 100% 5664/5667 [00:03<00:00, 1776.16 uttr/s, accuracy=0.70, loss=1.33]\nTrain: 100% 2000/2000 [00:41<00:00, 47.72 step/s, accuracy=0.59, loss=1.00, step=86000]\nValid: 100% 5664/5667 [00:03<00:00, 1696.91 uttr/s, accuracy=0.71, loss=1.30]\nTrain: 100% 2000/2000 [00:41<00:00, 48.41 step/s, accuracy=0.81, loss=1.05, step=88000]\nValid: 100% 5664/5667 [00:03<00:00, 1838.36 uttr/s, accuracy=0.70, loss=1.32]\nTrain: 100% 2000/2000 [00:41<00:00, 47.64 step/s, accuracy=0.81, loss=0.54, step=9e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1811.23 uttr/s, accuracy=0.71, loss=1.29]\nTrain:   1% 11/2000 [00:00<00:36, 53.91 step/s, accuracy=0.78, loss=0.71, step=9e+4]","output_type":"stream"},{"name":"stdout","text":"Step 90000, best model saved. (accuracy=0.7092)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:40<00:00, 49.08 step/s, accuracy=0.75, loss=1.26, step=92000]\nValid: 100% 5664/5667 [00:03<00:00, 1807.83 uttr/s, accuracy=0.70, loss=1.29]\nTrain: 100% 2000/2000 [00:41<00:00, 47.65 step/s, accuracy=0.69, loss=0.99, step=94000]\nValid: 100% 5664/5667 [00:03<00:00, 1842.27 uttr/s, accuracy=0.71, loss=1.31]\nTrain: 100% 2000/2000 [00:44<00:00, 45.40 step/s, accuracy=0.84, loss=0.55, step=96000]\nValid: 100% 5664/5667 [00:03<00:00, 1689.23 uttr/s, accuracy=0.70, loss=1.33]\nTrain: 100% 2000/2000 [00:41<00:00, 47.99 step/s, accuracy=0.75, loss=1.00, step=98000]\nValid: 100% 5664/5667 [00:03<00:00, 1561.12 uttr/s, accuracy=0.70, loss=1.31]\nTrain: 100% 2000/2000 [00:42<00:00, 47.32 step/s, accuracy=0.78, loss=0.95, step=1e+5] \nValid: 100% 5664/5667 [00:03<00:00, 1632.75 uttr/s, accuracy=0.71, loss=1.26]\nTrain:   0% 0/2000 [00:00<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"Step 100000, best model saved. (accuracy=0.7135)\n","output_type":"stream"}]},{"cell_type":"code","source":"def parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"/kaggle/input/cs4650dataset/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 100000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    transformer_model = TransformerClassifier(n_spks=speaker_num).to(device)\n    criterion = AMSoftmax()\n    optimizer = AdamW(transformer_model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, transformer_model, criterion, device, True)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata transformer model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, transformer_model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = transformer_model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-05-02T15:13:42.336631Z","iopub.execute_input":"2023-05-02T15:13:42.337541Z","iopub.status.idle":"2023-05-02T15:54:39.579516Z","shell.execute_reply.started":"2023-05-02T15:13:42.337482Z","shell.execute_reply":"2023-05-02T15:54:39.578295Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train:   0% 0/2000 [00:00<?, ? step/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\nTrain: 100% 2000/2000 [00:42<00:00, 46.61 step/s, accuracy=0.03, loss=17.11, step=2000]\nValid: 100% 5664/5667 [00:03<00:00, 1822.04 uttr/s, accuracy=0.03, loss=9.02]\nTrain: 100% 2000/2000 [00:43<00:00, 46.28 step/s, accuracy=0.06, loss=16.33, step=4000]\nValid: 100% 5664/5667 [00:03<00:00, 1786.90 uttr/s, accuracy=0.08, loss=8.53]\nTrain: 100% 2000/2000 [00:42<00:00, 46.53 step/s, accuracy=0.12, loss=16.05, step=6000]\nValid: 100% 5664/5667 [00:03<00:00, 1753.44 uttr/s, accuracy=0.13, loss=8.21]\nTrain: 100% 2000/2000 [00:43<00:00, 45.83 step/s, accuracy=0.19, loss=15.76, step=8000]\nValid: 100% 5664/5667 [00:03<00:00, 1695.48 uttr/s, accuracy=0.20, loss=7.94]\nTrain: 100% 2000/2000 [00:42<00:00, 46.75 step/s, accuracy=0.28, loss=14.36, step=1e+4]\nValid: 100% 5664/5667 [00:03<00:00, 1720.33 uttr/s, accuracy=0.24, loss=7.65]\nTrain:   0% 9/2000 [00:00<00:38, 52.28 step/s, accuracy=0.31, loss=14.76, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.2426)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:44<00:00, 44.46 step/s, accuracy=0.22, loss=15.59, step=12000]\nValid: 100% 5664/5667 [00:03<00:00, 1737.86 uttr/s, accuracy=0.27, loss=7.33]\nTrain: 100% 2000/2000 [00:46<00:00, 43.20 step/s, accuracy=0.31, loss=14.85, step=14000]\nValid: 100% 5664/5667 [00:03<00:00, 1619.81 uttr/s, accuracy=0.30, loss=7.10]\nTrain: 100% 2000/2000 [00:44<00:00, 45.22 step/s, accuracy=0.34, loss=13.92, step=16000]\nValid: 100% 5664/5667 [00:03<00:00, 1772.92 uttr/s, accuracy=0.36, loss=6.71]\nTrain: 100% 2000/2000 [00:43<00:00, 46.37 step/s, accuracy=0.50, loss=12.60, step=18000]\nValid: 100% 5664/5667 [00:03<00:00, 1759.22 uttr/s, accuracy=0.39, loss=6.49]\nTrain: 100% 2000/2000 [00:43<00:00, 46.13 step/s, accuracy=0.47, loss=12.66, step=2e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1765.30 uttr/s, accuracy=0.41, loss=6.16]\nTrain:   0% 9/2000 [00:00<00:36, 54.07 step/s, accuracy=0.47, loss=12.73, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.4107)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:42<00:00, 47.07 step/s, accuracy=0.31, loss=15.23, step=22000]\nValid: 100% 5664/5667 [00:03<00:00, 1756.30 uttr/s, accuracy=0.42, loss=6.03]\nTrain: 100% 2000/2000 [00:46<00:00, 43.46 step/s, accuracy=0.44, loss=13.29, step=24000]\nValid: 100% 5664/5667 [00:03<00:00, 1808.04 uttr/s, accuracy=0.42, loss=5.85]\nTrain: 100% 2000/2000 [00:46<00:00, 42.66 step/s, accuracy=0.38, loss=14.12, step=26000]\nValid: 100% 5664/5667 [00:03<00:00, 1736.87 uttr/s, accuracy=0.45, loss=5.58]\nTrain: 100% 2000/2000 [00:46<00:00, 43.42 step/s, accuracy=0.66, loss=9.12, step=28000] \nValid: 100% 5664/5667 [00:03<00:00, 1781.73 uttr/s, accuracy=0.48, loss=5.44]\nTrain: 100% 2000/2000 [00:48<00:00, 41.40 step/s, accuracy=0.53, loss=11.39, step=3e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1750.39 uttr/s, accuracy=0.49, loss=5.31]\nTrain:   0% 10/2000 [00:00<00:37, 53.06 step/s, accuracy=0.53, loss=11.36, step=3e+4]","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.4880)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:45<00:00, 43.82 step/s, accuracy=0.66, loss=9.38, step=32000] \nValid: 100% 5664/5667 [00:03<00:00, 1798.78 uttr/s, accuracy=0.52, loss=5.03]\nTrain: 100% 2000/2000 [00:47<00:00, 41.89 step/s, accuracy=0.62, loss=8.50, step=34000] \nValid: 100% 5664/5667 [00:03<00:00, 1488.17 uttr/s, accuracy=0.51, loss=4.99]\nTrain: 100% 2000/2000 [00:48<00:00, 40.84 step/s, accuracy=0.62, loss=12.33, step=36000]\nValid: 100% 5664/5667 [00:03<00:00, 1721.35 uttr/s, accuracy=0.52, loss=4.82]\nTrain: 100% 2000/2000 [00:52<00:00, 37.83 step/s, accuracy=0.47, loss=12.27, step=38000]\nValid: 100% 5664/5667 [00:03<00:00, 1715.10 uttr/s, accuracy=0.55, loss=4.73]\nTrain: 100% 2000/2000 [00:49<00:00, 40.39 step/s, accuracy=0.75, loss=8.39, step=4e+4]  \nValid: 100% 5664/5667 [00:07<00:00, 746.69 uttr/s, accuracy=0.55, loss=4.69] \nTrain:   0% 10/2000 [00:00<00:37, 53.03 step/s, accuracy=0.72, loss=7.94, step=4e+4]","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.5493)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:47<00:00, 42.36 step/s, accuracy=0.59, loss=9.84, step=42000] \nValid: 100% 5664/5667 [00:03<00:00, 1705.47 uttr/s, accuracy=0.56, loss=4.54]\nTrain: 100% 2000/2000 [00:51<00:00, 38.97 step/s, accuracy=0.66, loss=9.37, step=44000] \nValid: 100% 5664/5667 [00:05<00:00, 1028.38 uttr/s, accuracy=0.57, loss=4.45]\nTrain: 100% 2000/2000 [00:46<00:00, 42.84 step/s, accuracy=0.53, loss=12.37, step=46000]\nValid: 100% 5664/5667 [00:03<00:00, 1636.09 uttr/s, accuracy=0.58, loss=4.28]\nTrain: 100% 2000/2000 [00:45<00:00, 43.87 step/s, accuracy=0.53, loss=11.32, step=48000]\nValid: 100% 5664/5667 [00:03<00:00, 1713.81 uttr/s, accuracy=0.60, loss=4.17]\nTrain: 100% 2000/2000 [00:49<00:00, 40.79 step/s, accuracy=0.72, loss=8.64, step=5e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1776.97 uttr/s, accuracy=0.60, loss=4.13]\nTrain:   0% 7/2000 [00:00<01:04, 30.86 step/s, accuracy=0.50, loss=11.78, step=5e+4]","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.6003)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:45<00:00, 43.72 step/s, accuracy=0.62, loss=8.96, step=52000] \nValid: 100% 5664/5667 [00:03<00:00, 1788.30 uttr/s, accuracy=0.61, loss=3.97]\nTrain: 100% 2000/2000 [00:43<00:00, 46.24 step/s, accuracy=0.72, loss=8.13, step=54000] \nValid: 100% 5664/5667 [00:03<00:00, 1770.89 uttr/s, accuracy=0.62, loss=3.97]\nTrain: 100% 2000/2000 [00:43<00:00, 46.36 step/s, accuracy=0.59, loss=9.52, step=56000] \nValid: 100% 5664/5667 [00:03<00:00, 1780.02 uttr/s, accuracy=0.62, loss=3.89]\nTrain: 100% 2000/2000 [00:43<00:00, 45.78 step/s, accuracy=0.78, loss=6.96, step=58000] \nValid: 100% 5664/5667 [00:03<00:00, 1709.45 uttr/s, accuracy=0.62, loss=3.87]\nTrain: 100% 2000/2000 [00:43<00:00, 46.40 step/s, accuracy=0.53, loss=10.92, step=6e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1744.26 uttr/s, accuracy=0.64, loss=3.79]\nTrain:   0% 10/2000 [00:00<00:40, 49.44 step/s, accuracy=0.78, loss=7.47, step=6e+4]","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.6379)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:43<00:00, 45.63 step/s, accuracy=0.72, loss=8.15, step=62000] \nValid: 100% 5664/5667 [00:03<00:00, 1796.69 uttr/s, accuracy=0.64, loss=3.70]\nTrain: 100% 2000/2000 [00:43<00:00, 46.00 step/s, accuracy=0.62, loss=8.89, step=64000] \nValid: 100% 5664/5667 [00:03<00:00, 1761.52 uttr/s, accuracy=0.65, loss=3.63]\nTrain: 100% 2000/2000 [00:42<00:00, 46.57 step/s, accuracy=0.72, loss=8.28, step=66000] \nValid: 100% 5664/5667 [00:03<00:00, 1687.65 uttr/s, accuracy=0.65, loss=3.60]\nTrain: 100% 2000/2000 [00:42<00:00, 46.68 step/s, accuracy=0.75, loss=7.22, step=68000] \nValid: 100% 5664/5667 [00:03<00:00, 1716.12 uttr/s, accuracy=0.66, loss=3.54]\nTrain: 100% 2000/2000 [00:42<00:00, 46.77 step/s, accuracy=0.66, loss=9.05, step=7e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1504.10 uttr/s, accuracy=0.67, loss=3.46]\nTrain:   1% 11/2000 [00:00<00:29, 68.46 step/s, accuracy=0.78, loss=6.03, step=7e+4]","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.6730)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:42<00:00, 46.76 step/s, accuracy=0.81, loss=7.12, step=72000] \nValid: 100% 5664/5667 [00:03<00:00, 1801.64 uttr/s, accuracy=0.66, loss=3.52]\nTrain: 100% 2000/2000 [00:43<00:00, 46.49 step/s, accuracy=0.69, loss=7.96, step=74000] \nValid: 100% 5664/5667 [00:03<00:00, 1796.31 uttr/s, accuracy=0.68, loss=3.39]\nTrain: 100% 2000/2000 [00:43<00:00, 46.41 step/s, accuracy=0.69, loss=8.50, step=76000] \nValid: 100% 5664/5667 [00:03<00:00, 1732.14 uttr/s, accuracy=0.68, loss=3.39]\nTrain: 100% 2000/2000 [00:43<00:00, 46.19 step/s, accuracy=0.81, loss=6.06, step=78000] \nValid: 100% 5664/5667 [00:03<00:00, 1757.06 uttr/s, accuracy=0.68, loss=3.36]\nTrain: 100% 2000/2000 [00:43<00:00, 46.15 step/s, accuracy=0.84, loss=4.76, step=8e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1778.61 uttr/s, accuracy=0.67, loss=3.35]\nTrain:   0% 10/2000 [00:00<00:35, 56.11 step/s, accuracy=0.81, loss=5.78, step=8e+4]","output_type":"stream"},{"name":"stdout","text":"Step 80000, best model saved. (accuracy=0.6838)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:42<00:00, 46.87 step/s, accuracy=0.69, loss=8.98, step=82000] \nValid: 100% 5664/5667 [00:03<00:00, 1812.85 uttr/s, accuracy=0.68, loss=3.29]\nTrain: 100% 2000/2000 [00:43<00:00, 46.49 step/s, accuracy=0.72, loss=8.77, step=84000] \nValid: 100% 5664/5667 [00:03<00:00, 1787.25 uttr/s, accuracy=0.69, loss=3.27]\nTrain: 100% 2000/2000 [00:42<00:00, 47.04 step/s, accuracy=0.69, loss=7.20, step=86000] \nValid: 100% 5664/5667 [00:03<00:00, 1779.66 uttr/s, accuracy=0.69, loss=3.25]\nTrain: 100% 2000/2000 [00:44<00:00, 45.44 step/s, accuracy=0.84, loss=6.49, step=88000] \nValid: 100% 5664/5667 [00:03<00:00, 1775.37 uttr/s, accuracy=0.69, loss=3.25]\nTrain: 100% 2000/2000 [00:55<00:00, 35.76 step/s, accuracy=0.75, loss=7.74, step=9e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1797.73 uttr/s, accuracy=0.69, loss=3.29]\nTrain:   0% 7/2000 [00:00<01:10, 28.19 step/s, accuracy=0.78, loss=5.57, step=9e+4]","output_type":"stream"},{"name":"stdout","text":"Step 90000, best model saved. (accuracy=0.6944)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:49<00:00, 40.32 step/s, accuracy=0.84, loss=5.58, step=92000] \nValid: 100% 5664/5667 [00:03<00:00, 1802.34 uttr/s, accuracy=0.68, loss=3.27]\nTrain: 100% 2000/2000 [00:53<00:00, 37.14 step/s, accuracy=0.84, loss=6.65, step=94000] \nValid: 100% 5664/5667 [00:03<00:00, 1785.38 uttr/s, accuracy=0.70, loss=3.20]\nTrain: 100% 2000/2000 [00:48<00:00, 41.33 step/s, accuracy=0.72, loss=6.09, step=96000] \nValid: 100% 5664/5667 [00:03<00:00, 1528.41 uttr/s, accuracy=0.69, loss=3.23]\nTrain: 100% 2000/2000 [00:52<00:00, 37.82 step/s, accuracy=0.72, loss=7.20, step=98000] \nValid: 100% 5664/5667 [00:03<00:00, 1811.92 uttr/s, accuracy=0.70, loss=3.22]\nTrain: 100% 2000/2000 [00:51<00:00, 38.92 step/s, accuracy=0.78, loss=5.90, step=1e+5]  \nValid: 100% 5664/5667 [00:03<00:00, 1773.10 uttr/s, accuracy=0.69, loss=3.25]\nTrain:   0% 0/2000 [00:00<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"Step 100000, best model saved. (accuracy=0.7006)\n","output_type":"stream"}]},{"cell_type":"code","source":"def parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"/kaggle/input/cs4650dataset/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 100000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    transformer_model = TransformerClassifierPooling(n_spks=speaker_num).to(device)\n    criterion = CrossEntropyLoss()\n    optimizer = AdamW(transformer_model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, transformer_model, criterion, device)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata transformer model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, transformer_model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = transformer_model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-05-02T15:54:39.581807Z","iopub.execute_input":"2023-05-02T15:54:39.582223Z","iopub.status.idle":"2023-05-02T16:34:03.002624Z","shell.execute_reply.started":"2023-05-02T15:54:39.582176Z","shell.execute_reply":"2023-05-02T16:34:03.001233Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:55<00:00, 36.00 step/s, accuracy=0.16, loss=3.60, step=2000]\nValid: 100% 5664/5667 [00:04<00:00, 1265.40 uttr/s, accuracy=0.22, loss=3.83]\nTrain: 100% 2000/2000 [00:47<00:00, 42.38 step/s, accuracy=0.44, loss=3.08, step=4000]\nValid: 100% 5664/5667 [00:04<00:00, 1170.53 uttr/s, accuracy=0.38, loss=2.87]\nTrain: 100% 2000/2000 [00:50<00:00, 39.52 step/s, accuracy=0.50, loss=2.06, step=6000]\nValid: 100% 5664/5667 [00:03<00:00, 1800.93 uttr/s, accuracy=0.46, loss=2.45]\nTrain: 100% 2000/2000 [00:49<00:00, 40.70 step/s, accuracy=0.62, loss=1.56, step=8000]\nValid: 100% 5664/5667 [00:03<00:00, 1797.76 uttr/s, accuracy=0.51, loss=2.19]\nTrain: 100% 2000/2000 [00:51<00:00, 39.13 step/s, accuracy=0.44, loss=2.62, step=1e+4]\nValid: 100% 5664/5667 [00:03<00:00, 1515.03 uttr/s, accuracy=0.54, loss=2.05]\nTrain:   1% 11/2000 [00:00<00:33, 58.55 step/s, accuracy=0.56, loss=1.68, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.5355)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:45<00:00, 43.74 step/s, accuracy=0.47, loss=2.18, step=12000]\nValid: 100% 5664/5667 [00:03<00:00, 1796.23 uttr/s, accuracy=0.56, loss=1.95]\nTrain: 100% 2000/2000 [00:44<00:00, 45.19 step/s, accuracy=0.66, loss=1.91, step=14000]\nValid: 100% 5664/5667 [00:03<00:00, 1739.33 uttr/s, accuracy=0.58, loss=1.81]\nTrain: 100% 2000/2000 [00:42<00:00, 47.52 step/s, accuracy=0.72, loss=1.32, step=16000]\nValid: 100% 5664/5667 [00:03<00:00, 1737.40 uttr/s, accuracy=0.62, loss=1.67]\nTrain: 100% 2000/2000 [00:41<00:00, 48.51 step/s, accuracy=0.69, loss=1.55, step=18000]\nValid: 100% 5664/5667 [00:03<00:00, 1599.07 uttr/s, accuracy=0.62, loss=1.64]\nTrain: 100% 2000/2000 [00:41<00:00, 48.04 step/s, accuracy=0.59, loss=1.83, step=2e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1784.12 uttr/s, accuracy=0.63, loss=1.62]\nTrain:   0% 10/2000 [00:00<00:36, 54.32 step/s, accuracy=0.59, loss=1.65, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.6287)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:41<00:00, 48.40 step/s, accuracy=0.62, loss=1.22, step=22000]\nValid: 100% 5664/5667 [00:03<00:00, 1798.31 uttr/s, accuracy=0.64, loss=1.54]\nTrain: 100% 2000/2000 [00:42<00:00, 47.34 step/s, accuracy=0.72, loss=1.00, step=24000]\nValid: 100% 5664/5667 [00:03<00:00, 1795.28 uttr/s, accuracy=0.64, loss=1.55]\nTrain: 100% 2000/2000 [00:41<00:00, 48.06 step/s, accuracy=0.75, loss=1.34, step=26000]\nValid: 100% 5664/5667 [00:03<00:00, 1828.55 uttr/s, accuracy=0.66, loss=1.50]\nTrain: 100% 2000/2000 [00:41<00:00, 48.74 step/s, accuracy=0.56, loss=1.84, step=28000]\nValid: 100% 5664/5667 [00:03<00:00, 1544.59 uttr/s, accuracy=0.66, loss=1.46]\nTrain: 100% 2000/2000 [00:41<00:00, 48.56 step/s, accuracy=0.66, loss=1.61, step=3e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1813.38 uttr/s, accuracy=0.67, loss=1.45]\nTrain:   0% 10/2000 [00:00<00:37, 52.50 step/s, accuracy=0.78, loss=0.92, step=3e+4]","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.6697)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:41<00:00, 48.07 step/s, accuracy=0.62, loss=1.36, step=32000]\nValid: 100% 5664/5667 [00:03<00:00, 1820.37 uttr/s, accuracy=0.68, loss=1.41]\nTrain: 100% 2000/2000 [00:42<00:00, 47.45 step/s, accuracy=0.75, loss=0.85, step=34000]\nValid: 100% 5664/5667 [00:03<00:00, 1817.77 uttr/s, accuracy=0.68, loss=1.39]\nTrain: 100% 2000/2000 [00:43<00:00, 46.22 step/s, accuracy=0.66, loss=1.08, step=36000]\nValid: 100% 5664/5667 [00:03<00:00, 1693.40 uttr/s, accuracy=0.68, loss=1.43]\nTrain: 100% 2000/2000 [00:47<00:00, 42.37 step/s, accuracy=0.66, loss=1.28, step=38000]\nValid: 100% 5664/5667 [00:03<00:00, 1817.79 uttr/s, accuracy=0.70, loss=1.34]\nTrain: 100% 2000/2000 [00:46<00:00, 43.08 step/s, accuracy=0.84, loss=0.69, step=4e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1363.45 uttr/s, accuracy=0.70, loss=1.33]\nTrain:   0% 10/2000 [00:00<00:33, 58.90 step/s, accuracy=0.72, loss=0.89, step=4e+4]","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.7013)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:57<00:00, 35.06 step/s, accuracy=0.75, loss=1.26, step=42000]\nValid: 100% 5664/5667 [00:03<00:00, 1810.87 uttr/s, accuracy=0.70, loss=1.33]\nTrain: 100% 2000/2000 [00:43<00:00, 45.82 step/s, accuracy=0.75, loss=0.88, step=44000]\nValid: 100% 5664/5667 [00:03<00:00, 1771.19 uttr/s, accuracy=0.71, loss=1.29]\nTrain: 100% 2000/2000 [00:41<00:00, 48.53 step/s, accuracy=0.69, loss=1.02, step=46000]\nValid: 100% 5664/5667 [00:03<00:00, 1708.09 uttr/s, accuracy=0.71, loss=1.25]\nTrain: 100% 2000/2000 [00:41<00:00, 48.28 step/s, accuracy=0.78, loss=0.89, step=48000]\nValid: 100% 5664/5667 [00:03<00:00, 1484.42 uttr/s, accuracy=0.70, loss=1.28]\nTrain: 100% 2000/2000 [00:41<00:00, 48.09 step/s, accuracy=0.69, loss=1.28, step=5e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1821.56 uttr/s, accuracy=0.72, loss=1.26]\nTrain:   0% 10/2000 [00:00<00:37, 52.56 step/s, accuracy=0.81, loss=0.53, step=5e+4]","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.7161)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:41<00:00, 48.00 step/s, accuracy=0.78, loss=0.93, step=52000]\nValid: 100% 5664/5667 [00:03<00:00, 1823.91 uttr/s, accuracy=0.73, loss=1.19]\nTrain: 100% 2000/2000 [00:43<00:00, 46.34 step/s, accuracy=0.88, loss=0.36, step=54000]\nValid: 100% 5664/5667 [00:03<00:00, 1812.33 uttr/s, accuracy=0.73, loss=1.20]\nTrain: 100% 2000/2000 [00:41<00:00, 48.16 step/s, accuracy=0.81, loss=0.63, step=56000]\nValid: 100% 5664/5667 [00:03<00:00, 1688.97 uttr/s, accuracy=0.71, loss=1.26]\nTrain: 100% 2000/2000 [00:41<00:00, 47.77 step/s, accuracy=0.78, loss=0.94, step=58000]\nValid: 100% 5664/5667 [00:03<00:00, 1809.26 uttr/s, accuracy=0.73, loss=1.17]\nTrain: 100% 2000/2000 [00:41<00:00, 48.18 step/s, accuracy=0.72, loss=1.18, step=6e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1832.26 uttr/s, accuracy=0.73, loss=1.17]\nTrain:   0% 10/2000 [00:00<00:36, 55.18 step/s, accuracy=0.72, loss=0.96, step=6e+4]","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.7323)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:41<00:00, 48.21 step/s, accuracy=0.88, loss=0.39, step=62000]\nValid: 100% 5664/5667 [00:03<00:00, 1809.88 uttr/s, accuracy=0.73, loss=1.16]\nTrain: 100% 2000/2000 [00:42<00:00, 47.48 step/s, accuracy=0.94, loss=0.33, step=64000]\nValid: 100% 5664/5667 [00:03<00:00, 1705.02 uttr/s, accuracy=0.73, loss=1.18]\nTrain: 100% 2000/2000 [00:40<00:00, 49.00 step/s, accuracy=0.75, loss=1.35, step=66000]\nValid: 100% 5664/5667 [00:03<00:00, 1726.86 uttr/s, accuracy=0.74, loss=1.12]\nTrain: 100% 2000/2000 [00:41<00:00, 47.97 step/s, accuracy=0.84, loss=0.42, step=68000]\nValid: 100% 5664/5667 [00:03<00:00, 1763.49 uttr/s, accuracy=0.75, loss=1.10]\nTrain: 100% 2000/2000 [00:40<00:00, 48.79 step/s, accuracy=0.94, loss=0.39, step=7e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1741.86 uttr/s, accuracy=0.74, loss=1.13]\nTrain:   0% 10/2000 [00:00<00:36, 54.93 step/s, accuracy=0.75, loss=0.87, step=7e+4]","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.7521)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:41<00:00, 48.11 step/s, accuracy=0.84, loss=0.83, step=72000]\nValid: 100% 5664/5667 [00:03<00:00, 1705.10 uttr/s, accuracy=0.74, loss=1.11]\nTrain: 100% 2000/2000 [00:40<00:00, 49.68 step/s, accuracy=0.84, loss=0.67, step=74000]\nValid: 100% 5664/5667 [00:03<00:00, 1796.57 uttr/s, accuracy=0.75, loss=1.12]\nTrain: 100% 2000/2000 [00:40<00:00, 49.84 step/s, accuracy=0.91, loss=0.41, step=76000]\nValid: 100% 5664/5667 [00:03<00:00, 1771.24 uttr/s, accuracy=0.75, loss=1.07]\nTrain: 100% 2000/2000 [00:40<00:00, 49.29 step/s, accuracy=0.88, loss=0.41, step=78000]\nValid: 100% 5664/5667 [00:03<00:00, 1772.96 uttr/s, accuracy=0.75, loss=1.07]\nTrain: 100% 2000/2000 [00:40<00:00, 49.24 step/s, accuracy=0.91, loss=0.55, step=8e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1805.09 uttr/s, accuracy=0.76, loss=1.06]\nTrain:   0% 10/2000 [00:00<00:39, 50.57 step/s, accuracy=0.84, loss=0.63, step=8e+4]","output_type":"stream"},{"name":"stdout","text":"Step 80000, best model saved. (accuracy=0.7622)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:40<00:00, 49.80 step/s, accuracy=0.81, loss=0.69, step=82000]\nValid: 100% 5664/5667 [00:03<00:00, 1747.79 uttr/s, accuracy=0.76, loss=1.07]\nTrain: 100% 2000/2000 [00:44<00:00, 44.61 step/s, accuracy=0.75, loss=1.06, step=84000]\nValid: 100% 5664/5667 [00:03<00:00, 1829.51 uttr/s, accuracy=0.76, loss=1.04]\nTrain: 100% 2000/2000 [00:44<00:00, 45.00 step/s, accuracy=0.78, loss=0.48, step=86000]\nValid: 100% 5664/5667 [00:03<00:00, 1831.96 uttr/s, accuracy=0.76, loss=1.06]\nTrain: 100% 2000/2000 [00:44<00:00, 45.12 step/s, accuracy=0.81, loss=0.58, step=88000]\nValid: 100% 5664/5667 [00:03<00:00, 1676.91 uttr/s, accuracy=0.77, loss=1.05]\nTrain: 100% 2000/2000 [00:47<00:00, 42.10 step/s, accuracy=0.81, loss=0.64, step=9e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1855.84 uttr/s, accuracy=0.76, loss=1.06]\nTrain:   0% 10/2000 [00:00<00:39, 50.39 step/s, accuracy=0.84, loss=0.79, step=9e+4]","output_type":"stream"},{"name":"stdout","text":"Step 90000, best model saved. (accuracy=0.7662)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:45<00:00, 44.25 step/s, accuracy=0.88, loss=0.49, step=92000]\nValid: 100% 5664/5667 [00:03<00:00, 1762.96 uttr/s, accuracy=0.77, loss=1.02]\nTrain: 100% 2000/2000 [00:44<00:00, 44.92 step/s, accuracy=0.78, loss=0.78, step=94000]\nValid: 100% 5664/5667 [00:05<00:00, 1013.00 uttr/s, accuracy=0.76, loss=1.04]\nTrain: 100% 2000/2000 [00:43<00:00, 46.01 step/s, accuracy=0.94, loss=0.35, step=96000]\nValid: 100% 5664/5667 [00:03<00:00, 1639.06 uttr/s, accuracy=0.77, loss=1.03]\nTrain: 100% 2000/2000 [00:48<00:00, 41.44 step/s, accuracy=0.91, loss=0.51, step=98000]\nValid: 100% 5664/5667 [00:03<00:00, 1822.00 uttr/s, accuracy=0.77, loss=1.04]\nTrain: 100% 2000/2000 [00:47<00:00, 41.81 step/s, accuracy=0.84, loss=0.46, step=1e+5] \nValid: 100% 5664/5667 [00:05<00:00, 1043.76 uttr/s, accuracy=0.77, loss=1.02]\nTrain:   0% 0/2000 [00:00<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"Step 100000, best model saved. (accuracy=0.7724)\n","output_type":"stream"}]},{"cell_type":"code","source":"def parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"/kaggle/input/cs4650dataset/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 100000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    transformer_model = TransformerClassifierPooling(n_spks=speaker_num).to(device)\n    criterion = AMSoftmax()\n    optimizer = AdamW(transformer_model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, transformer_model, criterion, device, True)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata transformer model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, transformer_model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = transformer_model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-05-02T16:34:03.004930Z","iopub.execute_input":"2023-05-02T16:34:03.005560Z","iopub.status.idle":"2023-05-02T17:17:42.688692Z","shell.execute_reply.started":"2023-05-02T16:34:03.005510Z","shell.execute_reply":"2023-05-02T17:17:42.687540Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train:   0% 0/2000 [00:00<?, ? step/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\nTrain: 100% 2000/2000 [00:48<00:00, 41.65 step/s, accuracy=0.06, loss=17.42, step=2000]\nValid: 100% 5664/5667 [00:03<00:00, 1712.28 uttr/s, accuracy=0.06, loss=8.88]\nTrain: 100% 2000/2000 [01:18<00:00, 25.58 step/s, accuracy=0.28, loss=15.26, step=4000]\nValid: 100% 5664/5667 [00:18<00:00, 308.01 uttr/s, accuracy=0.27, loss=7.70]\nTrain: 100% 2000/2000 [01:10<00:00, 28.18 step/s, accuracy=0.53, loss=12.58, step=6000]\nValid: 100% 5664/5667 [00:03<00:00, 1699.15 uttr/s, accuracy=0.38, loss=6.71]\nTrain: 100% 2000/2000 [00:43<00:00, 45.63 step/s, accuracy=0.59, loss=11.71, step=8000]\nValid: 100% 5664/5667 [00:03<00:00, 1797.59 uttr/s, accuracy=0.47, loss=5.79]\nTrain: 100% 2000/2000 [00:47<00:00, 42.12 step/s, accuracy=0.44, loss=13.52, step=1e+4]\nValid: 100% 5664/5667 [00:03<00:00, 1774.97 uttr/s, accuracy=0.52, loss=5.13]\nTrain:   0% 10/2000 [00:00<00:39, 50.56 step/s, accuracy=0.59, loss=12.20, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.5175)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:52<00:00, 38.28 step/s, accuracy=0.50, loss=11.99, step=12000]\nValid: 100% 5664/5667 [00:03<00:00, 1523.57 uttr/s, accuracy=0.56, loss=4.66]\nTrain: 100% 2000/2000 [00:48<00:00, 40.97 step/s, accuracy=0.62, loss=11.83, step=14000]\nValid: 100% 5664/5667 [00:03<00:00, 1780.10 uttr/s, accuracy=0.58, loss=4.57]\nTrain: 100% 2000/2000 [00:52<00:00, 38.06 step/s, accuracy=0.78, loss=8.38, step=16000] \nValid: 100% 5664/5667 [00:07<00:00, 716.67 uttr/s, accuracy=0.59, loss=4.27] \nTrain: 100% 2000/2000 [00:50<00:00, 39.82 step/s, accuracy=0.62, loss=9.23, step=18000] \nValid: 100% 5664/5667 [00:03<00:00, 1768.39 uttr/s, accuracy=0.61, loss=4.04]\nTrain: 100% 2000/2000 [00:47<00:00, 42.04 step/s, accuracy=0.81, loss=5.88, step=2e+4]  \nValid: 100% 5664/5667 [00:04<00:00, 1164.51 uttr/s, accuracy=0.62, loss=3.93]\nTrain:   0% 10/2000 [00:00<00:38, 51.35 step/s, accuracy=0.66, loss=8.28, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.6194)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:47<00:00, 42.19 step/s, accuracy=0.59, loss=8.28, step=22000] \nValid: 100% 5664/5667 [00:03<00:00, 1705.38 uttr/s, accuracy=0.63, loss=3.81]\nTrain: 100% 2000/2000 [00:48<00:00, 40.88 step/s, accuracy=0.62, loss=10.06, step=24000]\nValid: 100% 5664/5667 [00:03<00:00, 1773.74 uttr/s, accuracy=0.65, loss=3.70]\nTrain: 100% 2000/2000 [00:43<00:00, 45.88 step/s, accuracy=0.75, loss=7.75, step=26000] \nValid: 100% 5664/5667 [00:03<00:00, 1742.06 uttr/s, accuracy=0.64, loss=3.66]\nTrain: 100% 2000/2000 [00:54<00:00, 36.64 step/s, accuracy=0.81, loss=9.38, step=28000] \nValid: 100% 5664/5667 [00:03<00:00, 1537.22 uttr/s, accuracy=0.66, loss=3.55]\nTrain: 100% 2000/2000 [00:43<00:00, 46.09 step/s, accuracy=0.75, loss=7.28, step=3e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1790.54 uttr/s, accuracy=0.66, loss=3.53]\nTrain:   0% 10/2000 [00:00<00:37, 53.25 step/s, accuracy=0.81, loss=6.39, step=3e+4]","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.6619)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:43<00:00, 46.17 step/s, accuracy=0.84, loss=6.22, step=32000] \nValid: 100% 5664/5667 [00:03<00:00, 1794.74 uttr/s, accuracy=0.67, loss=3.44]\nTrain: 100% 2000/2000 [00:42<00:00, 46.83 step/s, accuracy=0.72, loss=7.83, step=34000] \nValid: 100% 5664/5667 [00:03<00:00, 1742.25 uttr/s, accuracy=0.67, loss=3.38]\nTrain: 100% 2000/2000 [00:42<00:00, 46.71 step/s, accuracy=0.72, loss=7.00, step=36000] \nValid: 100% 5664/5667 [00:03<00:00, 1729.78 uttr/s, accuracy=0.68, loss=3.33]\nTrain: 100% 2000/2000 [00:42<00:00, 46.77 step/s, accuracy=0.72, loss=8.42, step=38000] \nValid: 100% 5664/5667 [00:03<00:00, 1835.84 uttr/s, accuracy=0.69, loss=3.26]\nTrain: 100% 2000/2000 [00:42<00:00, 46.86 step/s, accuracy=0.53, loss=10.01, step=4e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1535.75 uttr/s, accuracy=0.70, loss=3.21]\nTrain:   0% 10/2000 [00:00<00:36, 54.11 step/s, accuracy=0.81, loss=4.87, step=4e+4]","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.6954)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:42<00:00, 47.19 step/s, accuracy=0.78, loss=7.96, step=42000] \nValid: 100% 5664/5667 [00:03<00:00, 1778.43 uttr/s, accuracy=0.71, loss=3.08]\nTrain: 100% 2000/2000 [00:42<00:00, 47.19 step/s, accuracy=0.75, loss=5.86, step=44000] \nValid: 100% 5664/5667 [00:03<00:00, 1812.19 uttr/s, accuracy=0.71, loss=3.06]\nTrain: 100% 2000/2000 [00:47<00:00, 42.29 step/s, accuracy=0.88, loss=6.85, step=46000] \nValid: 100% 5664/5667 [00:03<00:00, 1817.15 uttr/s, accuracy=0.72, loss=2.99]\nTrain: 100% 2000/2000 [00:42<00:00, 46.73 step/s, accuracy=0.78, loss=6.01, step=48000] \nValid: 100% 5664/5667 [00:03<00:00, 1775.86 uttr/s, accuracy=0.71, loss=3.03]\nTrain: 100% 2000/2000 [00:42<00:00, 46.76 step/s, accuracy=0.88, loss=4.87, step=5e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1758.75 uttr/s, accuracy=0.72, loss=2.90]\nTrain:   0% 10/2000 [00:00<00:36, 54.38 step/s, accuracy=0.69, loss=10.76, step=5e+4]","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.7230)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:42<00:00, 47.23 step/s, accuracy=0.91, loss=4.29, step=52000] \nValid: 100% 5664/5667 [00:03<00:00, 1815.25 uttr/s, accuracy=0.72, loss=2.92]\nTrain: 100% 2000/2000 [00:44<00:00, 44.99 step/s, accuracy=0.69, loss=8.46, step=54000] \nValid: 100% 5664/5667 [00:03<00:00, 1788.85 uttr/s, accuracy=0.72, loss=2.90]\nTrain: 100% 2000/2000 [00:42<00:00, 46.72 step/s, accuracy=0.78, loss=5.56, step=56000] \nValid: 100% 5664/5667 [00:03<00:00, 1811.97 uttr/s, accuracy=0.73, loss=2.80]\nTrain: 100% 2000/2000 [00:43<00:00, 46.34 step/s, accuracy=0.94, loss=3.43, step=58000] \nValid: 100% 5664/5667 [00:03<00:00, 1713.19 uttr/s, accuracy=0.73, loss=2.85]\nTrain: 100% 2000/2000 [00:42<00:00, 46.82 step/s, accuracy=0.75, loss=5.19, step=6e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1812.91 uttr/s, accuracy=0.74, loss=2.74]\nTrain:   0% 10/2000 [00:00<00:41, 48.34 step/s, accuracy=0.88, loss=3.81, step=6e+4]","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.7431)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:43<00:00, 46.00 step/s, accuracy=0.81, loss=5.60, step=62000] \nValid: 100% 5664/5667 [00:03<00:00, 1472.07 uttr/s, accuracy=0.74, loss=2.74]\nTrain: 100% 2000/2000 [00:43<00:00, 45.91 step/s, accuracy=0.72, loss=6.70, step=64000] \nValid: 100% 5664/5667 [00:03<00:00, 1791.93 uttr/s, accuracy=0.75, loss=2.70]\nTrain: 100% 2000/2000 [00:48<00:00, 41.62 step/s, accuracy=0.75, loss=6.28, step=66000] \nValid: 100% 5664/5667 [00:03<00:00, 1546.30 uttr/s, accuracy=0.74, loss=2.73]\nTrain: 100% 2000/2000 [00:48<00:00, 41.24 step/s, accuracy=0.75, loss=5.77, step=68000] \nValid: 100% 5664/5667 [00:05<00:00, 1095.93 uttr/s, accuracy=0.76, loss=2.63]\nTrain: 100% 2000/2000 [00:48<00:00, 41.04 step/s, accuracy=0.69, loss=6.37, step=7e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1809.13 uttr/s, accuracy=0.75, loss=2.62]\nTrain:   0% 10/2000 [00:00<00:35, 55.94 step/s, accuracy=0.75, loss=6.74, step=7e+4]","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.7558)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:51<00:00, 39.09 step/s, accuracy=0.81, loss=5.00, step=72000] \nValid: 100% 5664/5667 [00:03<00:00, 1600.55 uttr/s, accuracy=0.76, loss=2.59]\nTrain: 100% 2000/2000 [00:47<00:00, 41.81 step/s, accuracy=0.69, loss=7.85, step=74000] \nValid: 100% 5664/5667 [00:04<00:00, 1220.16 uttr/s, accuracy=0.75, loss=2.55]\nTrain: 100% 2000/2000 [00:48<00:00, 41.50 step/s, accuracy=0.78, loss=5.09, step=76000] \nValid: 100% 5664/5667 [00:03<00:00, 1727.00 uttr/s, accuracy=0.76, loss=2.55]\nTrain: 100% 2000/2000 [01:18<00:00, 25.33 step/s, accuracy=0.81, loss=4.83, step=78000] \nValid: 100% 5664/5667 [00:07<00:00, 740.20 uttr/s, accuracy=0.77, loss=2.50] \nTrain: 100% 2000/2000 [01:02<00:00, 31.76 step/s, accuracy=0.91, loss=4.59, step=8e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1717.69 uttr/s, accuracy=0.77, loss=2.50]\nTrain:   0% 9/2000 [00:00<00:42, 47.35 step/s, accuracy=0.88, loss=4.21, step=8e+4]","output_type":"stream"},{"name":"stdout","text":"Step 80000, best model saved. (accuracy=0.7678)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:46<00:00, 43.10 step/s, accuracy=0.72, loss=8.73, step=82000] \nValid: 100% 5664/5667 [00:03<00:00, 1772.73 uttr/s, accuracy=0.76, loss=2.50]\nTrain: 100% 2000/2000 [00:50<00:00, 39.46 step/s, accuracy=0.81, loss=6.15, step=84000] \nValid: 100% 5664/5667 [00:03<00:00, 1772.19 uttr/s, accuracy=0.77, loss=2.51]\nTrain: 100% 2000/2000 [00:48<00:00, 41.44 step/s, accuracy=0.84, loss=4.40, step=86000] \nValid: 100% 5664/5667 [00:04<00:00, 1141.27 uttr/s, accuracy=0.77, loss=2.47]\nTrain: 100% 2000/2000 [00:47<00:00, 41.93 step/s, accuracy=0.81, loss=5.58, step=88000] \nValid: 100% 5664/5667 [00:03<00:00, 1731.20 uttr/s, accuracy=0.78, loss=2.39]\nTrain: 100% 2000/2000 [00:48<00:00, 40.93 step/s, accuracy=0.81, loss=5.51, step=9e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1810.04 uttr/s, accuracy=0.77, loss=2.42]\nTrain:   0% 10/2000 [00:00<00:39, 50.24 step/s, accuracy=0.78, loss=5.78, step=9e+4]","output_type":"stream"},{"name":"stdout","text":"Step 90000, best model saved. (accuracy=0.7809)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [00:46<00:00, 43.09 step/s, accuracy=0.84, loss=4.15, step=92000] \nValid: 100% 5664/5667 [00:04<00:00, 1345.88 uttr/s, accuracy=0.78, loss=2.39]\nTrain: 100% 2000/2000 [00:47<00:00, 42.53 step/s, accuracy=0.78, loss=6.04, step=94000]\nValid: 100% 5664/5667 [00:03<00:00, 1765.27 uttr/s, accuracy=0.77, loss=2.45]\nTrain: 100% 2000/2000 [00:45<00:00, 43.70 step/s, accuracy=0.94, loss=2.81, step=96000]\nValid: 100% 5664/5667 [00:04<00:00, 1304.30 uttr/s, accuracy=0.77, loss=2.44]\nTrain: 100% 2000/2000 [00:47<00:00, 42.37 step/s, accuracy=0.84, loss=4.85, step=98000] \nValid: 100% 5664/5667 [00:03<00:00, 1551.03 uttr/s, accuracy=0.78, loss=2.42]\nTrain: 100% 2000/2000 [00:46<00:00, 42.87 step/s, accuracy=0.84, loss=4.21, step=1e+5]  \nValid: 100% 5664/5667 [00:03<00:00, 1752.07 uttr/s, accuracy=0.78, loss=2.40]\nTrain:   0% 0/2000 [00:00<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"Step 100000, best model saved. (accuracy=0.7809)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conformer Model Train","metadata":{}},{"cell_type":"code","source":"def parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"../input/cs4650dataset/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 100000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    conformer_model = ConformerClassifier(n_spks=speaker_num).to(device)\n    criterion = CrossEntropyLoss()\n    optimizer = AdamW(conformer_model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, conformer_model, criterion, device)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata transformer model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, conformer_model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = conformer_model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"id":"Usv9s-CuJSG7","execution":{"iopub.status.busy":"2023-05-02T00:42:00.003422Z","iopub.execute_input":"2023-05-02T00:42:00.004183Z","iopub.status.idle":"2023-05-02T02:07:55.290138Z","shell.execute_reply.started":"2023-05-02T00:42:00.004137Z","shell.execute_reply":"2023-05-02T02:07:55.288980Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"},{"name":"stdout","text":"[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:54<00:00, 17.47 step/s, accuracy=0.06, loss=4.24, step=2000]\nValid: 100% 5664/5667 [00:06<00:00, 838.89 uttr/s, accuracy=0.14, loss=4.15] \nTrain: 100% 2000/2000 [01:34<00:00, 21.09 step/s, accuracy=0.31, loss=3.27, step=4000]\nValid: 100% 5664/5667 [00:04<00:00, 1345.04 uttr/s, accuracy=0.24, loss=3.39]\nTrain: 100% 2000/2000 [01:34<00:00, 21.22 step/s, accuracy=0.41, loss=2.49, step=6000]\nValid: 100% 5664/5667 [00:04<00:00, 1344.76 uttr/s, accuracy=0.31, loss=3.01]\nTrain: 100% 2000/2000 [01:34<00:00, 21.10 step/s, accuracy=0.47, loss=2.70, step=8000]\nValid: 100% 5664/5667 [00:04<00:00, 1182.18 uttr/s, accuracy=0.38, loss=2.68]\nTrain: 100% 2000/2000 [01:37<00:00, 20.60 step/s, accuracy=0.38, loss=2.40, step=1e+4]\nValid: 100% 5664/5667 [00:04<00:00, 1357.64 uttr/s, accuracy=0.39, loss=2.63]\nTrain:   0% 4/2000 [00:00<01:53, 17.65 step/s, accuracy=0.19, loss=3.03, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.3928)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:34<00:00, 21.05 step/s, accuracy=0.59, loss=1.91, step=12000]\nValid: 100% 5664/5667 [00:04<00:00, 1370.51 uttr/s, accuracy=0.47, loss=2.25]\nTrain: 100% 2000/2000 [01:35<00:00, 20.98 step/s, accuracy=0.38, loss=2.33, step=14000]\nValid: 100% 5664/5667 [00:04<00:00, 1364.97 uttr/s, accuracy=0.49, loss=2.18]\nTrain: 100% 2000/2000 [01:35<00:00, 20.87 step/s, accuracy=0.59, loss=1.76, step=16000]\nValid: 100% 5664/5667 [00:04<00:00, 1246.09 uttr/s, accuracy=0.53, loss=2.00]\nTrain: 100% 2000/2000 [01:41<00:00, 19.64 step/s, accuracy=0.59, loss=1.71, step=18000]\nValid: 100% 5664/5667 [00:08<00:00, 657.99 uttr/s, accuracy=0.53, loss=2.01] \nTrain: 100% 2000/2000 [01:47<00:00, 18.69 step/s, accuracy=0.75, loss=1.15, step=2e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1367.98 uttr/s, accuracy=0.56, loss=1.87]\nTrain:   0% 4/2000 [00:00<01:49, 18.22 step/s, accuracy=0.59, loss=1.55, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.5569)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.43 step/s, accuracy=0.53, loss=1.82, step=22000]\nValid: 100% 5664/5667 [00:04<00:00, 1338.63 uttr/s, accuracy=0.58, loss=1.79]\nTrain: 100% 2000/2000 [01:37<00:00, 20.59 step/s, accuracy=0.59, loss=1.83, step=28000]\nValid: 100% 5664/5667 [00:04<00:00, 1407.47 uttr/s, accuracy=0.60, loss=1.65]\nTrain:  10% 200/2000 [00:09<01:36, 18.58 step/s, accuracy=0.62, loss=1.45, step=28200]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nTrain: 100% 2000/2000 [01:36<00:00, 20.71 step/s, accuracy=0.69, loss=0.95, step=3e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1402.39 uttr/s, accuracy=0.63, loss=1.57]\nTrain:   0% 4/2000 [00:00<01:47, 18.65 step/s, accuracy=0.81, loss=0.85, step=3e+4]","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.6301)\n","output_type":"stream"},{"name":"stderr","text":"Train:  58% 1156/2000 [00:55<00:38, 21.79 step/s, accuracy=0.62, loss=1.34, step=31156]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nValid: 100% 5664/5667 [00:04<00:00, 1338.45 uttr/s, accuracy=0.66, loss=1.46]\nTrain: 100% 2000/2000 [01:41<00:00, 19.77 step/s, accuracy=0.66, loss=1.49, step=34000]\nValid: 100% 5664/5667 [00:04<00:00, 1350.88 uttr/s, accuracy=0.64, loss=1.51]\nTrain: 100% 2000/2000 [01:37<00:00, 20.57 step/s, accuracy=0.84, loss=0.78, step=36000]\nValid: 100% 5664/5667 [00:04<00:00, 1391.48 uttr/s, accuracy=0.66, loss=1.43]\nTrain: 100% 2000/2000 [01:36<00:00, 20.72 step/s, accuracy=0.75, loss=1.60, step=38000]\nValid: 100% 5664/5667 [00:04<00:00, 1394.15 uttr/s, accuracy=0.68, loss=1.36]\nTrain: 100% 2000/2000 [01:38<00:00, 20.37 step/s, accuracy=0.81, loss=0.69, step=4e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1308.50 uttr/s, accuracy=0.68, loss=1.34]\nTrain:   0% 3/2000 [00:00<01:54, 17.47 step/s, accuracy=0.84, loss=0.61, step=4e+4]","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.6838)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:39<00:00, 20.03 step/s, accuracy=0.62, loss=1.40, step=42000]\nValid: 100% 5664/5667 [00:04<00:00, 1189.25 uttr/s, accuracy=0.69, loss=1.28]\nTrain: 100% 2000/2000 [01:38<00:00, 20.40 step/s, accuracy=0.81, loss=1.12, step=44000]\nValid: 100% 5664/5667 [00:04<00:00, 1376.12 uttr/s, accuracy=0.70, loss=1.27]\nTrain: 100% 2000/2000 [01:40<00:00, 19.89 step/s, accuracy=0.66, loss=1.43, step=46000]\nValid: 100% 5664/5667 [00:04<00:00, 1255.99 uttr/s, accuracy=0.70, loss=1.27]\nTrain: 100% 2000/2000 [01:44<00:00, 19.09 step/s, accuracy=0.69, loss=0.84, step=48000]\nValid: 100% 5664/5667 [00:05<00:00, 1074.12 uttr/s, accuracy=0.71, loss=1.21]\nTrain: 100% 2000/2000 [01:42<00:00, 19.56 step/s, accuracy=0.88, loss=0.36, step=5e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1255.31 uttr/s, accuracy=0.71, loss=1.19]\nTrain:   0% 4/2000 [00:00<01:56, 17.16 step/s, accuracy=0.84, loss=0.72, step=5e+4]","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.7126)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:39<00:00, 20.02 step/s, accuracy=0.78, loss=0.88, step=52000]\nValid: 100% 5664/5667 [00:04<00:00, 1335.89 uttr/s, accuracy=0.73, loss=1.14]\nTrain: 100% 2000/2000 [01:38<00:00, 20.39 step/s, accuracy=0.88, loss=0.44, step=54000]\nValid: 100% 5664/5667 [00:04<00:00, 1330.43 uttr/s, accuracy=0.72, loss=1.19]\nTrain: 100% 2000/2000 [01:39<00:00, 20.00 step/s, accuracy=0.94, loss=0.38, step=56000]\nValid: 100% 5664/5667 [00:04<00:00, 1344.41 uttr/s, accuracy=0.74, loss=1.12]\nTrain: 100% 2000/2000 [01:36<00:00, 20.62 step/s, accuracy=0.78, loss=0.71, step=58000]\nValid: 100% 5664/5667 [00:04<00:00, 1349.12 uttr/s, accuracy=0.74, loss=1.08]\nTrain: 100% 2000/2000 [01:37<00:00, 20.59 step/s, accuracy=0.84, loss=0.48, step=6e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1360.76 uttr/s, accuracy=0.75, loss=1.05]\nTrain:   0% 4/2000 [00:00<01:49, 18.18 step/s, accuracy=0.78, loss=0.66, step=6e+4]","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.7486)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.54 step/s, accuracy=0.84, loss=0.49, step=62000]\nValid: 100% 5664/5667 [00:04<00:00, 1337.76 uttr/s, accuracy=0.75, loss=1.05]\nTrain: 100% 2000/2000 [01:39<00:00, 20.08 step/s, accuracy=0.81, loss=0.68, step=64000]\nValid: 100% 5664/5667 [00:04<00:00, 1361.38 uttr/s, accuracy=0.76, loss=1.04]\nTrain: 100% 2000/2000 [01:37<00:00, 20.62 step/s, accuracy=0.81, loss=0.64, step=66000]\nValid: 100% 5664/5667 [00:04<00:00, 1257.81 uttr/s, accuracy=0.75, loss=1.04]\nTrain: 100% 2000/2000 [01:39<00:00, 20.13 step/s, accuracy=0.75, loss=0.99, step=68000]\nValid: 100% 5664/5667 [00:04<00:00, 1292.06 uttr/s, accuracy=0.77, loss=0.99]\nTrain: 100% 2000/2000 [01:37<00:00, 20.46 step/s, accuracy=0.88, loss=0.45, step=7e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1366.03 uttr/s, accuracy=0.77, loss=0.98]\nTrain:   0% 4/2000 [00:00<01:46, 18.77 step/s, accuracy=0.84, loss=0.42, step=7e+4]","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.7710)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.43 step/s, accuracy=0.91, loss=0.31, step=72000]\nValid: 100% 5664/5667 [00:04<00:00, 1309.69 uttr/s, accuracy=0.78, loss=0.95]\nTrain: 100% 2000/2000 [01:36<00:00, 20.63 step/s, accuracy=0.91, loss=0.39, step=74000]\nValid: 100% 5664/5667 [00:04<00:00, 1346.28 uttr/s, accuracy=0.78, loss=0.96]\nTrain: 100% 2000/2000 [01:37<00:00, 20.44 step/s, accuracy=0.94, loss=0.38, step=76000]\nValid: 100% 5664/5667 [00:04<00:00, 1343.87 uttr/s, accuracy=0.78, loss=0.94]\nTrain: 100% 2000/2000 [01:37<00:00, 20.46 step/s, accuracy=0.84, loss=0.47, step=78000]\nValid: 100% 5664/5667 [00:04<00:00, 1378.53 uttr/s, accuracy=0.78, loss=0.95]\nTrain: 100% 2000/2000 [01:37<00:00, 20.45 step/s, accuracy=0.91, loss=0.39, step=8e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1353.58 uttr/s, accuracy=0.79, loss=0.91]\nTrain:   0% 4/2000 [00:00<01:46, 18.73 step/s, accuracy=0.91, loss=0.31, step=8e+4]","output_type":"stream"},{"name":"stdout","text":"Step 80000, best model saved. (accuracy=0.7873)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:38<00:00, 20.23 step/s, accuracy=0.97, loss=0.38, step=82000]\nValid: 100% 5664/5667 [00:05<00:00, 1016.04 uttr/s, accuracy=0.78, loss=0.92]\nTrain: 100% 2000/2000 [01:40<00:00, 19.84 step/s, accuracy=0.97, loss=0.16, step=84000]\nValid: 100% 5664/5667 [00:04<00:00, 1307.80 uttr/s, accuracy=0.78, loss=0.91]\nTrain: 100% 2000/2000 [01:38<00:00, 20.37 step/s, accuracy=0.88, loss=0.59, step=86000]\nValid: 100% 5664/5667 [00:04<00:00, 1293.04 uttr/s, accuracy=0.79, loss=0.92]\nTrain: 100% 2000/2000 [01:39<00:00, 20.14 step/s, accuracy=0.88, loss=0.43, step=88000]\nValid: 100% 5664/5667 [00:04<00:00, 1352.06 uttr/s, accuracy=0.80, loss=0.89]\nTrain: 100% 2000/2000 [01:38<00:00, 20.21 step/s, accuracy=0.81, loss=0.51, step=9e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1312.68 uttr/s, accuracy=0.79, loss=0.93]\nTrain:   0% 3/2000 [00:00<02:02, 16.30 step/s, accuracy=0.91, loss=0.39, step=9e+4]","output_type":"stream"},{"name":"stdout","text":"Step 90000, best model saved. (accuracy=0.7994)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:39<00:00, 20.01 step/s, accuracy=1.00, loss=0.19, step=92000]\nValid: 100% 5664/5667 [00:04<00:00, 1224.35 uttr/s, accuracy=0.80, loss=0.87]\nTrain: 100% 2000/2000 [01:38<00:00, 20.36 step/s, accuracy=0.97, loss=0.15, step=94000]\nValid: 100% 5664/5667 [00:04<00:00, 1334.09 uttr/s, accuracy=0.80, loss=0.87]\nTrain: 100% 2000/2000 [01:37<00:00, 20.41 step/s, accuracy=0.84, loss=0.58, step=96000]\nValid: 100% 5664/5667 [00:04<00:00, 1336.38 uttr/s, accuracy=0.80, loss=0.89]\nTrain: 100% 2000/2000 [01:37<00:00, 20.51 step/s, accuracy=0.94, loss=0.23, step=98000]\nValid: 100% 5664/5667 [00:04<00:00, 1301.96 uttr/s, accuracy=0.79, loss=0.87]\nTrain: 100% 2000/2000 [01:37<00:00, 20.56 step/s, accuracy=0.81, loss=0.60, step=1e+5] \nValid: 100% 5664/5667 [00:04<00:00, 1358.10 uttr/s, accuracy=0.79, loss=0.89]\nTrain:   0% 0/2000 [00:00<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"Step 100000, best model saved. (accuracy=0.8003)\n","output_type":"stream"}]},{"cell_type":"code","source":"def parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"../input/cs4650dataset/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 100000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    conformer_model = ConformerClassifier(n_spks=speaker_num).to(device)\n    criterion = AMSoftmax()\n    optimizer = AdamW(conformer_model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, conformer_model, criterion, device, True)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata transformer model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, conformer_model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = conformer_model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-05-02T02:10:26.622205Z","iopub.execute_input":"2023-05-02T02:10:26.622630Z","iopub.status.idle":"2023-05-02T03:36:18.703821Z","shell.execute_reply.started":"2023-05-02T02:10:26.622586Z","shell.execute_reply":"2023-05-02T03:36:18.702440Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train:   0% 0/2000 [00:00<?, ? step/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\nTrain: 100% 2000/2000 [01:39<00:00, 20.05 step/s, accuracy=0.03, loss=17.23, step=2000]\nValid: 100% 5664/5667 [00:04<00:00, 1196.39 uttr/s, accuracy=0.02, loss=9.09]\nTrain: 100% 2000/2000 [01:38<00:00, 20.27 step/s, accuracy=0.03, loss=17.03, step=4000]\nValid: 100% 5664/5667 [00:04<00:00, 1311.13 uttr/s, accuracy=0.04, loss=8.80]\nTrain: 100% 2000/2000 [01:39<00:00, 20.14 step/s, accuracy=0.03, loss=17.03, step=6000]\nValid: 100% 5664/5667 [00:04<00:00, 1300.56 uttr/s, accuracy=0.05, loss=8.67]\nTrain: 100% 2000/2000 [01:39<00:00, 20.14 step/s, accuracy=0.03, loss=16.95, step=8000]\nValid: 100% 5664/5667 [00:04<00:00, 1317.68 uttr/s, accuracy=0.05, loss=8.50]\nTrain: 100% 2000/2000 [01:40<00:00, 19.88 step/s, accuracy=0.00, loss=16.32, step=1e+4]\nValid: 100% 5664/5667 [00:05<00:00, 1127.93 uttr/s, accuracy=0.08, loss=8.42]\nTrain:   0% 3/2000 [00:00<01:58, 16.81 step/s, accuracy=0.06, loss=16.20, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.0750)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:39<00:00, 20.05 step/s, accuracy=0.06, loss=16.17, step=12000]\nValid: 100% 5664/5667 [00:04<00:00, 1275.45 uttr/s, accuracy=0.07, loss=8.41]\nTrain: 100% 2000/2000 [01:39<00:00, 20.06 step/s, accuracy=0.06, loss=15.59, step=14000]\nValid: 100% 5664/5667 [00:04<00:00, 1334.72 uttr/s, accuracy=0.09, loss=8.33]\nTrain: 100% 2000/2000 [01:39<00:00, 20.12 step/s, accuracy=0.06, loss=16.46, step=16000]\nValid: 100% 5664/5667 [00:04<00:00, 1344.57 uttr/s, accuracy=0.09, loss=8.28]\nTrain: 100% 2000/2000 [01:40<00:00, 19.98 step/s, accuracy=0.12, loss=16.54, step=18000]\nValid: 100% 5664/5667 [00:04<00:00, 1327.28 uttr/s, accuracy=0.09, loss=8.15]\nTrain: 100% 2000/2000 [01:40<00:00, 19.87 step/s, accuracy=0.12, loss=16.22, step=2e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1289.06 uttr/s, accuracy=0.09, loss=8.17]\nTrain:   0% 3/2000 [00:00<02:00, 16.64 step/s, accuracy=0.09, loss=16.01, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.0927)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:40<00:00, 19.92 step/s, accuracy=0.09, loss=16.29, step=22000]\nValid: 100% 5664/5667 [00:04<00:00, 1307.64 uttr/s, accuracy=0.10, loss=8.13]\nTrain: 100% 2000/2000 [01:39<00:00, 20.17 step/s, accuracy=0.12, loss=15.51, step=24000]\nValid: 100% 5664/5667 [00:04<00:00, 1189.46 uttr/s, accuracy=0.11, loss=7.99]\nTrain: 100% 2000/2000 [01:36<00:00, 20.72 step/s, accuracy=0.00, loss=16.40, step=26000]\nValid: 100% 5664/5667 [00:04<00:00, 1293.37 uttr/s, accuracy=0.13, loss=7.92]\nTrain: 100% 2000/2000 [01:36<00:00, 20.78 step/s, accuracy=0.12, loss=15.99, step=28000]\nValid: 100% 5664/5667 [00:04<00:00, 1275.93 uttr/s, accuracy=0.14, loss=7.79]\nTrain: 100% 2000/2000 [01:36<00:00, 20.76 step/s, accuracy=0.22, loss=15.64, step=3e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1329.13 uttr/s, accuracy=0.16, loss=7.67]\nTrain:   0% 4/2000 [00:00<01:52, 17.69 step/s, accuracy=0.16, loss=15.30, step=3e+4]","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.1626)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.51 step/s, accuracy=0.19, loss=15.28, step=32000]\nValid: 100% 5664/5667 [00:04<00:00, 1335.06 uttr/s, accuracy=0.21, loss=7.50]\nTrain: 100% 2000/2000 [01:37<00:00, 20.51 step/s, accuracy=0.31, loss=14.63, step=34000]\nValid: 100% 5664/5667 [00:04<00:00, 1292.09 uttr/s, accuracy=0.24, loss=7.26]\nTrain: 100% 2000/2000 [01:38<00:00, 20.37 step/s, accuracy=0.34, loss=14.02, step=36000]\nValid: 100% 5664/5667 [00:04<00:00, 1142.89 uttr/s, accuracy=0.29, loss=6.94]\nTrain: 100% 2000/2000 [01:36<00:00, 20.70 step/s, accuracy=0.44, loss=12.73, step=38000]\nValid: 100% 5664/5667 [00:04<00:00, 1326.32 uttr/s, accuracy=0.32, loss=6.76]\nTrain: 100% 2000/2000 [01:38<00:00, 20.26 step/s, accuracy=0.28, loss=13.38, step=4e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1302.90 uttr/s, accuracy=0.34, loss=6.54]\nTrain:   0% 4/2000 [00:00<02:01, 16.41 step/s, accuracy=0.25, loss=14.71, step=4e+4]","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.3422)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:38<00:00, 20.28 step/s, accuracy=0.38, loss=13.10, step=42000]\nValid: 100% 5664/5667 [00:04<00:00, 1268.43 uttr/s, accuracy=0.38, loss=6.28]\nTrain: 100% 2000/2000 [01:38<00:00, 20.32 step/s, accuracy=0.44, loss=12.89, step=44000]\nValid: 100% 5664/5667 [00:04<00:00, 1274.42 uttr/s, accuracy=0.39, loss=5.98]\nTrain: 100% 2000/2000 [01:38<00:00, 20.29 step/s, accuracy=0.44, loss=13.08, step=46000]\nValid: 100% 5664/5667 [00:04<00:00, 1330.42 uttr/s, accuracy=0.42, loss=5.91]\nTrain: 100% 2000/2000 [01:37<00:00, 20.47 step/s, accuracy=0.44, loss=12.15, step=48000]\nValid: 100% 5664/5667 [00:04<00:00, 1287.51 uttr/s, accuracy=0.47, loss=5.41]\nTrain: 100% 2000/2000 [01:38<00:00, 20.35 step/s, accuracy=0.53, loss=12.54, step=5e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1291.39 uttr/s, accuracy=0.48, loss=5.24]\nTrain:   0% 3/2000 [00:00<02:01, 16.40 step/s, accuracy=0.41, loss=13.93, step=5e+4]","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.4806)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:38<00:00, 20.37 step/s, accuracy=0.69, loss=9.53, step=52000] \nValid: 100% 5664/5667 [00:04<00:00, 1323.87 uttr/s, accuracy=0.50, loss=5.00]\nTrain: 100% 2000/2000 [01:38<00:00, 20.27 step/s, accuracy=0.59, loss=10.60, step=54000]\nValid: 100% 5664/5667 [00:04<00:00, 1134.00 uttr/s, accuracy=0.52, loss=4.85]\nTrain: 100% 2000/2000 [01:38<00:00, 20.32 step/s, accuracy=0.69, loss=8.97, step=56000] \nValid: 100% 5664/5667 [00:04<00:00, 1328.88 uttr/s, accuracy=0.54, loss=4.72]\nTrain: 100% 2000/2000 [01:39<00:00, 20.06 step/s, accuracy=0.56, loss=8.55, step=58000] \nValid: 100% 5664/5667 [00:04<00:00, 1283.63 uttr/s, accuracy=0.56, loss=4.38]\nTrain: 100% 2000/2000 [01:39<00:00, 20.02 step/s, accuracy=0.62, loss=11.91, step=6e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1335.72 uttr/s, accuracy=0.57, loss=4.39]\nTrain:   0% 4/2000 [00:00<01:56, 17.15 step/s, accuracy=0.59, loss=10.24, step=6e+4]","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.5738)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:39<00:00, 20.06 step/s, accuracy=0.66, loss=10.04, step=62000]\nValid: 100% 5664/5667 [00:04<00:00, 1295.70 uttr/s, accuracy=0.58, loss=4.25]\nTrain: 100% 2000/2000 [01:39<00:00, 20.16 step/s, accuracy=0.62, loss=8.05, step=64000] \nValid: 100% 5664/5667 [00:04<00:00, 1336.66 uttr/s, accuracy=0.59, loss=4.15]\nTrain: 100% 2000/2000 [01:37<00:00, 20.57 step/s, accuracy=0.72, loss=7.82, step=66000] \nValid: 100% 5664/5667 [00:04<00:00, 1311.75 uttr/s, accuracy=0.60, loss=3.96]\nTrain: 100% 2000/2000 [01:37<00:00, 20.50 step/s, accuracy=0.62, loss=9.64, step=68000] \nValid: 100% 5664/5667 [00:04<00:00, 1303.92 uttr/s, accuracy=0.62, loss=3.90]\nTrain: 100% 2000/2000 [01:37<00:00, 20.54 step/s, accuracy=0.69, loss=8.69, step=7e+4]  \nValid: 100% 5664/5667 [00:04<00:00, 1269.94 uttr/s, accuracy=0.62, loss=3.93]\nTrain:   0% 2/2000 [00:00<03:21,  9.92 step/s, accuracy=0.78, loss=7.85, step=7e+4]","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.6236)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:38<00:00, 20.34 step/s, accuracy=0.72, loss=8.60, step=72000] \nValid: 100% 5664/5667 [00:04<00:00, 1319.56 uttr/s, accuracy=0.63, loss=3.77]\nTrain: 100% 2000/2000 [01:37<00:00, 20.51 step/s, accuracy=0.72, loss=7.08, step=74000] \nValid: 100% 5664/5667 [00:04<00:00, 1328.62 uttr/s, accuracy=0.64, loss=3.63]\nTrain: 100% 2000/2000 [01:37<00:00, 20.58 step/s, accuracy=0.66, loss=9.55, step=76000] \nValid: 100% 5664/5667 [00:04<00:00, 1302.29 uttr/s, accuracy=0.66, loss=3.52]\nTrain: 100% 2000/2000 [01:37<00:00, 20.53 step/s, accuracy=0.69, loss=9.96, step=78000] \nValid: 100% 5664/5667 [00:04<00:00, 1282.51 uttr/s, accuracy=0.66, loss=3.51]\nTrain: 100% 2000/2000 [01:38<00:00, 20.35 step/s, accuracy=0.81, loss=4.00, step=8e+4]  \nValid: 100% 5664/5667 [00:04<00:00, 1185.23 uttr/s, accuracy=0.66, loss=3.47]\nTrain:   0% 4/2000 [00:00<01:54, 17.36 step/s, accuracy=0.81, loss=4.85, step=8e+4]","output_type":"stream"},{"name":"stdout","text":"Step 80000, best model saved. (accuracy=0.6623)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:39<00:00, 20.15 step/s, accuracy=0.72, loss=6.95, step=82000] \nValid: 100% 5664/5667 [00:04<00:00, 1313.10 uttr/s, accuracy=0.67, loss=3.42]\nTrain: 100% 2000/2000 [01:37<00:00, 20.58 step/s, accuracy=0.69, loss=7.36, step=84000] \nValid: 100% 5664/5667 [00:04<00:00, 1289.17 uttr/s, accuracy=0.67, loss=3.41]\nTrain: 100% 2000/2000 [01:37<00:00, 20.46 step/s, accuracy=0.91, loss=5.34, step=86000] \nValid: 100% 5664/5667 [00:04<00:00, 1354.98 uttr/s, accuracy=0.68, loss=3.33]\nTrain: 100% 2000/2000 [01:39<00:00, 20.15 step/s, accuracy=0.81, loss=5.69, step=88000] \nValid: 100% 5664/5667 [00:04<00:00, 1262.61 uttr/s, accuracy=0.67, loss=3.33]\nTrain: 100% 2000/2000 [01:37<00:00, 20.45 step/s, accuracy=0.69, loss=7.12, step=9e+4]  \nValid: 100% 5664/5667 [00:04<00:00, 1182.28 uttr/s, accuracy=0.68, loss=3.30]\nTrain:   0% 4/2000 [00:00<01:55, 17.35 step/s, accuracy=0.88, loss=3.36, step=9e+4]","output_type":"stream"},{"name":"stdout","text":"Step 90000, best model saved. (accuracy=0.6847)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:38<00:00, 20.29 step/s, accuracy=0.62, loss=9.20, step=92000] \nValid: 100% 5664/5667 [00:04<00:00, 1321.86 uttr/s, accuracy=0.69, loss=3.26]\nTrain: 100% 2000/2000 [01:38<00:00, 20.38 step/s, accuracy=0.88, loss=4.88, step=94000] \nValid: 100% 5664/5667 [00:04<00:00, 1262.59 uttr/s, accuracy=0.69, loss=3.21]\nTrain: 100% 2000/2000 [01:37<00:00, 20.48 step/s, accuracy=0.78, loss=5.61, step=96000] \nValid: 100% 5664/5667 [00:04<00:00, 1270.39 uttr/s, accuracy=0.69, loss=3.24]\nTrain: 100% 2000/2000 [01:41<00:00, 19.72 step/s, accuracy=0.69, loss=7.96, step=98000] \nValid: 100% 5664/5667 [00:04<00:00, 1143.48 uttr/s, accuracy=0.68, loss=3.29]\nTrain: 100% 2000/2000 [01:41<00:00, 19.67 step/s, accuracy=0.72, loss=7.37, step=1e+5]  \nValid: 100% 5664/5667 [00:04<00:00, 1307.01 uttr/s, accuracy=0.69, loss=3.27]\nTrain:   0% 0/2000 [00:00<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"Step 100000, best model saved. (accuracy=0.6930)\n","output_type":"stream"}]},{"cell_type":"code","source":"def parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"../input/cs4650dataset/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 100000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    conformer_model = ConformerClassifierPooling(n_spks=speaker_num).to(device)\n    criterion = CrossEntropyLoss()\n    optimizer = AdamW(conformer_model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, conformer_model, criterion, device)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata transformer model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, conformer_model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = conformer_model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-05-02T05:36:59.129984Z","iopub.execute_input":"2023-05-02T05:36:59.131129Z","iopub.status.idle":"2023-05-02T07:03:01.042103Z","shell.execute_reply.started":"2023-05-02T05:36:59.131081Z","shell.execute_reply":"2023-05-02T07:03:01.040932Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.48 step/s, accuracy=0.16, loss=4.08, step=2000]\nValid: 100% 5664/5667 [00:04<00:00, 1166.03 uttr/s, accuracy=0.20, loss=3.79]\nTrain: 100% 2000/2000 [01:37<00:00, 20.54 step/s, accuracy=0.34, loss=3.09, step=4000]\nValid: 100% 5664/5667 [00:04<00:00, 1345.94 uttr/s, accuracy=0.38, loss=2.75]\nTrain: 100% 2000/2000 [01:36<00:00, 20.68 step/s, accuracy=0.53, loss=2.06, step=6000]\nValid: 100% 5664/5667 [00:04<00:00, 1345.32 uttr/s, accuracy=0.49, loss=2.23]\nTrain: 100% 2000/2000 [01:37<00:00, 20.54 step/s, accuracy=0.50, loss=2.11, step=8000]\nValid: 100% 5664/5667 [00:04<00:00, 1325.12 uttr/s, accuracy=0.55, loss=1.96]\nTrain: 100% 2000/2000 [01:38<00:00, 20.41 step/s, accuracy=0.72, loss=1.27, step=1e+4]\nValid: 100% 5664/5667 [00:04<00:00, 1307.04 uttr/s, accuracy=0.61, loss=1.63]\nTrain:   0% 4/2000 [00:00<01:50, 18.09 step/s, accuracy=0.69, loss=1.19, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.6130)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.53 step/s, accuracy=0.69, loss=1.59, step=12000]\nValid: 100% 5664/5667 [00:04<00:00, 1169.83 uttr/s, accuracy=0.63, loss=1.54]\nTrain: 100% 2000/2000 [01:37<00:00, 20.52 step/s, accuracy=0.59, loss=1.60, step=14000]\nValid: 100% 5664/5667 [00:04<00:00, 1365.93 uttr/s, accuracy=0.67, loss=1.36]\nTrain: 100% 2000/2000 [01:37<00:00, 20.42 step/s, accuracy=0.75, loss=1.43, step=16000]\nValid: 100% 5664/5667 [00:04<00:00, 1306.32 uttr/s, accuracy=0.69, loss=1.28]\nTrain: 100% 2000/2000 [01:37<00:00, 20.53 step/s, accuracy=0.69, loss=1.15, step=18000]\nValid: 100% 5664/5667 [00:04<00:00, 1309.89 uttr/s, accuracy=0.73, loss=1.15]\nTrain: 100% 2000/2000 [01:37<00:00, 20.42 step/s, accuracy=0.88, loss=0.46, step=2e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1328.25 uttr/s, accuracy=0.72, loss=1.16]\nTrain:   0% 4/2000 [00:00<01:54, 17.42 step/s, accuracy=0.81, loss=0.58, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.7256)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.43 step/s, accuracy=0.81, loss=0.83, step=22000]\nValid: 100% 5664/5667 [00:04<00:00, 1204.76 uttr/s, accuracy=0.75, loss=1.03]\nTrain: 100% 2000/2000 [01:38<00:00, 20.38 step/s, accuracy=0.91, loss=0.51, step=24000]\nValid: 100% 5664/5667 [00:04<00:00, 1321.42 uttr/s, accuracy=0.75, loss=1.04]\nTrain: 100% 2000/2000 [01:37<00:00, 20.61 step/s, accuracy=0.81, loss=1.02, step=26000]\nValid: 100% 5664/5667 [00:04<00:00, 1368.67 uttr/s, accuracy=0.77, loss=0.99]\nTrain: 100% 2000/2000 [01:36<00:00, 20.73 step/s, accuracy=0.84, loss=0.52, step=28000]\nValid: 100% 5664/5667 [00:04<00:00, 1348.89 uttr/s, accuracy=0.77, loss=0.94]\nTrain: 100% 2000/2000 [01:36<00:00, 20.75 step/s, accuracy=0.94, loss=0.25, step=3e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1376.68 uttr/s, accuracy=0.78, loss=0.91]\nTrain:   0% 4/2000 [00:00<01:53, 17.62 step/s, accuracy=0.69, loss=0.72, step=3e+4]","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.7782)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.58 step/s, accuracy=0.97, loss=0.20, step=32000]\nValid: 100% 5664/5667 [00:04<00:00, 1330.60 uttr/s, accuracy=0.80, loss=0.83]\nTrain: 100% 2000/2000 [01:36<00:00, 20.70 step/s, accuracy=0.88, loss=0.80, step=34000]\nValid: 100% 5664/5667 [00:04<00:00, 1239.74 uttr/s, accuracy=0.80, loss=0.84]\nTrain: 100% 2000/2000 [01:37<00:00, 20.56 step/s, accuracy=0.88, loss=0.44, step=36000]\nValid: 100% 5664/5667 [00:04<00:00, 1328.44 uttr/s, accuracy=0.80, loss=0.82]\nTrain: 100% 2000/2000 [01:36<00:00, 20.71 step/s, accuracy=0.81, loss=0.65, step=38000]\nValid: 100% 5664/5667 [00:04<00:00, 1340.61 uttr/s, accuracy=0.81, loss=0.84]\nTrain: 100% 2000/2000 [01:36<00:00, 20.66 step/s, accuracy=0.88, loss=0.28, step=4e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1334.09 uttr/s, accuracy=0.82, loss=0.76]\nTrain:   0% 4/2000 [00:00<02:00, 16.57 step/s, accuracy=0.91, loss=0.31, step=4e+4]","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.8231)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.49 step/s, accuracy=0.91, loss=0.38, step=42000]\nValid: 100% 5664/5667 [00:04<00:00, 1343.71 uttr/s, accuracy=0.82, loss=0.77]\nTrain: 100% 2000/2000 [01:38<00:00, 20.29 step/s, accuracy=0.88, loss=0.40, step=44000]\nValid: 100% 5664/5667 [00:05<00:00, 1129.43 uttr/s, accuracy=0.83, loss=0.73]\nTrain: 100% 2000/2000 [01:37<00:00, 20.43 step/s, accuracy=0.88, loss=0.33, step=46000]\nValid: 100% 5664/5667 [00:04<00:00, 1331.74 uttr/s, accuracy=0.82, loss=0.76]\nTrain: 100% 2000/2000 [01:40<00:00, 19.97 step/s, accuracy=0.94, loss=0.24, step=48000]\nValid: 100% 5664/5667 [00:04<00:00, 1301.43 uttr/s, accuracy=0.84, loss=0.70]\nTrain: 100% 2000/2000 [01:40<00:00, 19.95 step/s, accuracy=0.97, loss=0.12, step=5e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1304.44 uttr/s, accuracy=0.83, loss=0.74]\nTrain:   0% 3/2000 [00:00<02:01, 16.41 step/s, accuracy=0.84, loss=0.38, step=5e+4]","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.8409)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:40<00:00, 19.91 step/s, accuracy=0.97, loss=0.34, step=52000]\nValid: 100% 5664/5667 [00:04<00:00, 1261.04 uttr/s, accuracy=0.85, loss=0.66]\nTrain: 100% 2000/2000 [01:38<00:00, 20.22 step/s, accuracy=0.94, loss=0.26, step=54000]\nValid: 100% 5664/5667 [00:04<00:00, 1312.97 uttr/s, accuracy=0.86, loss=0.66]\nTrain: 100% 2000/2000 [01:40<00:00, 19.97 step/s, accuracy=1.00, loss=0.11, step=56000]\nValid: 100% 5664/5667 [00:04<00:00, 1289.10 uttr/s, accuracy=0.85, loss=0.64]\nTrain: 100% 2000/2000 [01:39<00:00, 20.10 step/s, accuracy=1.00, loss=0.07, step=58000]\nValid: 100% 5664/5667 [00:04<00:00, 1342.31 uttr/s, accuracy=0.86, loss=0.63]\nTrain: 100% 2000/2000 [01:38<00:00, 20.21 step/s, accuracy=1.00, loss=0.09, step=6e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1339.81 uttr/s, accuracy=0.85, loss=0.64]\nTrain:   0% 4/2000 [00:00<01:47, 18.48 step/s, accuracy=0.97, loss=0.05, step=6e+4]","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.8591)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:38<00:00, 20.35 step/s, accuracy=0.97, loss=0.07, step=62000]\nValid: 100% 5664/5667 [00:04<00:00, 1305.58 uttr/s, accuracy=0.86, loss=0.60]\nTrain: 100% 2000/2000 [01:39<00:00, 20.18 step/s, accuracy=0.97, loss=0.07, step=64000]\nValid: 100% 5664/5667 [00:04<00:00, 1323.23 uttr/s, accuracy=0.86, loss=0.60]\nTrain: 100% 2000/2000 [01:39<00:00, 20.09 step/s, accuracy=0.91, loss=0.29, step=66000]\nValid: 100% 5664/5667 [00:04<00:00, 1269.82 uttr/s, accuracy=0.86, loss=0.63]\nTrain: 100% 2000/2000 [01:39<00:00, 20.00 step/s, accuracy=0.94, loss=0.16, step=68000]\nValid: 100% 5664/5667 [00:04<00:00, 1335.92 uttr/s, accuracy=0.87, loss=0.60]\nTrain: 100% 2000/2000 [01:39<00:00, 20.11 step/s, accuracy=0.97, loss=0.22, step=7e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1315.30 uttr/s, accuracy=0.87, loss=0.58]\nTrain:   0% 4/2000 [00:00<01:46, 18.67 step/s, accuracy=0.94, loss=0.25, step=7e+4]","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.8711)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:40<00:00, 19.83 step/s, accuracy=1.00, loss=0.05, step=72000]\nValid: 100% 5664/5667 [00:04<00:00, 1329.35 uttr/s, accuracy=0.87, loss=0.59]\nTrain: 100% 2000/2000 [01:40<00:00, 19.89 step/s, accuracy=1.00, loss=0.04, step=74000]\nValid: 100% 5664/5667 [00:04<00:00, 1152.74 uttr/s, accuracy=0.87, loss=0.57]\nTrain: 100% 2000/2000 [01:40<00:00, 19.97 step/s, accuracy=1.00, loss=0.04, step=76000]\nValid: 100% 5664/5667 [00:04<00:00, 1283.32 uttr/s, accuracy=0.88, loss=0.57]\nTrain: 100% 2000/2000 [01:40<00:00, 19.93 step/s, accuracy=0.97, loss=0.11, step=78000]\nValid: 100% 5664/5667 [00:04<00:00, 1283.34 uttr/s, accuracy=0.89, loss=0.53]\nTrain: 100% 2000/2000 [01:40<00:00, 19.85 step/s, accuracy=1.00, loss=0.04, step=8e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1340.42 uttr/s, accuracy=0.89, loss=0.53]\nTrain:   0% 3/2000 [00:00<02:00, 16.59 step/s, accuracy=0.97, loss=0.14, step=8e+4]","output_type":"stream"},{"name":"stdout","text":"Step 80000, best model saved. (accuracy=0.8861)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:41<00:00, 19.77 step/s, accuracy=1.00, loss=0.05, step=82000]\nValid: 100% 5664/5667 [00:04<00:00, 1290.16 uttr/s, accuracy=0.88, loss=0.54]\nTrain: 100% 2000/2000 [01:42<00:00, 19.47 step/s, accuracy=0.97, loss=0.07, step=84000]\nValid: 100% 5664/5667 [00:04<00:00, 1317.46 uttr/s, accuracy=0.88, loss=0.54]\nTrain: 100% 2000/2000 [01:40<00:00, 19.95 step/s, accuracy=1.00, loss=0.02, step=86000]\nValid: 100% 5664/5667 [00:04<00:00, 1341.80 uttr/s, accuracy=0.88, loss=0.52]\nTrain: 100% 2000/2000 [01:41<00:00, 19.66 step/s, accuracy=1.00, loss=0.03, step=88000]\nValid: 100% 5664/5667 [00:04<00:00, 1280.98 uttr/s, accuracy=0.88, loss=0.52]\nTrain: 100% 2000/2000 [01:40<00:00, 19.84 step/s, accuracy=1.00, loss=0.02, step=9e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1300.39 uttr/s, accuracy=0.89, loss=0.51]\nTrain:   0% 3/2000 [00:00<01:56, 17.13 step/s, accuracy=0.94, loss=0.20, step=9e+4]","output_type":"stream"},{"name":"stdout","text":"Step 90000, best model saved. (accuracy=0.8909)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:41<00:00, 19.80 step/s, accuracy=0.94, loss=0.16, step=92000]\nValid: 100% 5664/5667 [00:04<00:00, 1303.69 uttr/s, accuracy=0.89, loss=0.53]\nTrain: 100% 2000/2000 [01:40<00:00, 19.85 step/s, accuracy=1.00, loss=0.02, step=94000]\nValid: 100% 5664/5667 [00:04<00:00, 1178.59 uttr/s, accuracy=0.89, loss=0.49]\nTrain: 100% 2000/2000 [01:39<00:00, 20.15 step/s, accuracy=0.97, loss=0.30, step=96000]\nValid: 100% 5664/5667 [00:04<00:00, 1300.72 uttr/s, accuracy=0.89, loss=0.49]\nTrain: 100% 2000/2000 [01:38<00:00, 20.22 step/s, accuracy=1.00, loss=0.02, step=98000]\nValid: 100% 5664/5667 [00:04<00:00, 1331.62 uttr/s, accuracy=0.89, loss=0.52]\nTrain: 100% 2000/2000 [01:38<00:00, 20.30 step/s, accuracy=1.00, loss=0.06, step=1e+5] \nValid: 100% 5664/5667 [00:04<00:00, 1288.19 uttr/s, accuracy=0.89, loss=0.50]\nTrain:   0% 0/2000 [00:00<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"Step 100000, best model saved. (accuracy=0.8928)\n","output_type":"stream"}]},{"cell_type":"code","source":"def parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"../input/cs4650dataset/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 100000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    conformer_model = ConformerClassifierPooling(n_spks=speaker_num).to(device)\n    criterion = AMSoftmax()\n    optimizer = AdamW(conformer_model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, conformer_model, criterion, device, True)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata transformer model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, conformer_model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = conformer_model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2023-05-02T04:06:40.209469Z","iopub.execute_input":"2023-05-02T04:06:40.209868Z","iopub.status.idle":"2023-05-02T05:33:23.149356Z","shell.execute_reply.started":"2023-05-02T04:06:40.209824Z","shell.execute_reply":"2023-05-02T05:33:23.148027Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train:   0% 0/2000 [00:00<?, ? step/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\nTrain: 100% 2000/2000 [01:42<00:00, 19.50 step/s, accuracy=0.25, loss=15.80, step=2000]\nValid: 100% 5664/5667 [00:05<00:00, 1130.81 uttr/s, accuracy=0.17, loss=8.35]\nTrain: 100% 2000/2000 [01:40<00:00, 19.92 step/s, accuracy=0.28, loss=13.81, step=4000]\nValid: 100% 5664/5667 [00:04<00:00, 1312.12 uttr/s, accuracy=0.33, loss=7.08]\nTrain: 100% 2000/2000 [01:39<00:00, 20.07 step/s, accuracy=0.38, loss=13.91, step=6000]\nValid: 100% 5664/5667 [00:04<00:00, 1234.41 uttr/s, accuracy=0.43, loss=6.17]\nTrain: 100% 2000/2000 [01:41<00:00, 19.69 step/s, accuracy=0.41, loss=13.86, step=8000]\nValid: 100% 5664/5667 [00:04<00:00, 1306.69 uttr/s, accuracy=0.54, loss=4.71]\nTrain: 100% 2000/2000 [01:43<00:00, 19.38 step/s, accuracy=0.56, loss=10.27, step=1e+4]\nValid: 100% 5664/5667 [00:04<00:00, 1271.40 uttr/s, accuracy=0.61, loss=4.01]\nTrain:   0% 3/2000 [00:00<01:57, 16.95 step/s, accuracy=0.66, loss=10.44, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.6132)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:41<00:00, 19.73 step/s, accuracy=0.66, loss=10.11, step=12000]\nValid: 100% 5664/5667 [00:04<00:00, 1296.14 uttr/s, accuracy=0.64, loss=3.64]\nTrain: 100% 2000/2000 [01:41<00:00, 19.68 step/s, accuracy=0.81, loss=6.21, step=14000] \nValid: 100% 5664/5667 [00:04<00:00, 1230.42 uttr/s, accuracy=0.67, loss=3.41]\nTrain: 100% 2000/2000 [01:42<00:00, 19.49 step/s, accuracy=0.72, loss=7.67, step=16000] \nValid: 100% 5664/5667 [00:04<00:00, 1293.30 uttr/s, accuracy=0.71, loss=2.96]\nTrain: 100% 2000/2000 [01:40<00:00, 19.93 step/s, accuracy=0.69, loss=9.02, step=18000] \nValid: 100% 5664/5667 [00:04<00:00, 1310.11 uttr/s, accuracy=0.72, loss=2.92]\nTrain: 100% 2000/2000 [01:38<00:00, 20.37 step/s, accuracy=0.66, loss=9.35, step=2e+4]  \nValid: 100% 5664/5667 [00:04<00:00, 1288.20 uttr/s, accuracy=0.74, loss=2.69]\nTrain:   0% 3/2000 [00:00<01:57, 17.06 step/s, accuracy=0.97, loss=3.49, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.7442)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.45 step/s, accuracy=0.78, loss=6.02, step=22000] \nValid: 100% 5664/5667 [00:04<00:00, 1172.30 uttr/s, accuracy=0.75, loss=2.58]\nTrain: 100% 2000/2000 [01:39<00:00, 20.11 step/s, accuracy=0.81, loss=5.24, step=24000] \nValid: 100% 5664/5667 [00:04<00:00, 1281.20 uttr/s, accuracy=0.75, loss=2.55]\nTrain: 100% 2000/2000 [01:39<00:00, 20.06 step/s, accuracy=0.88, loss=4.10, step=26000] \nValid: 100% 5664/5667 [00:04<00:00, 1325.73 uttr/s, accuracy=0.77, loss=2.35]\nTrain: 100% 2000/2000 [01:38<00:00, 20.28 step/s, accuracy=0.94, loss=3.70, step=28000] \nValid: 100% 5664/5667 [00:04<00:00, 1326.59 uttr/s, accuracy=0.77, loss=2.39]\nTrain: 100% 2000/2000 [01:39<00:00, 20.13 step/s, accuracy=0.88, loss=5.14, step=3e+4] \nValid: 100% 5664/5667 [00:05<00:00, 1124.63 uttr/s, accuracy=0.77, loss=2.32]\nTrain:   0% 4/2000 [00:00<01:59, 16.73 step/s, accuracy=0.84, loss=4.91, step=3e+4]","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.7722)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:38<00:00, 20.29 step/s, accuracy=0.69, loss=7.09, step=32000] \nValid: 100% 5664/5667 [00:04<00:00, 1339.82 uttr/s, accuracy=0.79, loss=2.15]\nTrain: 100% 2000/2000 [01:39<00:00, 20.16 step/s, accuracy=0.91, loss=3.11, step=34000]\nValid: 100% 5664/5667 [00:04<00:00, 1287.23 uttr/s, accuracy=0.80, loss=2.09]\nTrain: 100% 2000/2000 [01:40<00:00, 19.99 step/s, accuracy=0.84, loss=4.12, step=36000]\nValid: 100% 5664/5667 [00:04<00:00, 1342.13 uttr/s, accuracy=0.80, loss=2.06]\nTrain: 100% 2000/2000 [01:38<00:00, 20.40 step/s, accuracy=0.88, loss=3.99, step=38000] \nValid: 100% 5664/5667 [00:04<00:00, 1160.39 uttr/s, accuracy=0.79, loss=2.11]\nTrain: 100% 2000/2000 [01:38<00:00, 20.34 step/s, accuracy=0.88, loss=3.41, step=4e+4]  \nValid: 100% 5664/5667 [00:04<00:00, 1302.29 uttr/s, accuracy=0.82, loss=1.92]\nTrain:   0% 4/2000 [00:00<01:50, 18.01 step/s, accuracy=0.78, loss=5.19, step=4e+4]","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.8190)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:37<00:00, 20.46 step/s, accuracy=0.91, loss=2.60, step=42000]\nValid: 100% 5664/5667 [00:04<00:00, 1307.45 uttr/s, accuracy=0.82, loss=1.88]\nTrain: 100% 2000/2000 [01:38<00:00, 20.28 step/s, accuracy=0.91, loss=5.30, step=44000]\nValid: 100% 5664/5667 [00:04<00:00, 1308.97 uttr/s, accuracy=0.82, loss=1.92]\nTrain: 100% 2000/2000 [01:39<00:00, 20.16 step/s, accuracy=0.84, loss=3.83, step=46000]\nValid: 100% 5664/5667 [00:04<00:00, 1261.75 uttr/s, accuracy=0.83, loss=1.80]\nTrain: 100% 2000/2000 [01:40<00:00, 19.97 step/s, accuracy=0.84, loss=3.00, step=48000]\nValid: 100% 5664/5667 [00:04<00:00, 1329.05 uttr/s, accuracy=0.83, loss=1.78]\nTrain: 100% 2000/2000 [01:38<00:00, 20.24 step/s, accuracy=0.94, loss=3.04, step=5e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1279.81 uttr/s, accuracy=0.84, loss=1.69]\nTrain:   0% 4/2000 [00:00<01:57, 16.99 step/s, accuracy=0.88, loss=3.35, step=5e+4]","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.8407)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:38<00:00, 20.26 step/s, accuracy=0.88, loss=2.98, step=52000]\nValid: 100% 5664/5667 [00:04<00:00, 1313.72 uttr/s, accuracy=0.84, loss=1.65]\nTrain: 100% 2000/2000 [01:39<00:00, 20.18 step/s, accuracy=1.00, loss=0.74, step=54000]\nValid: 100% 5664/5667 [00:04<00:00, 1159.18 uttr/s, accuracy=0.85, loss=1.63]\nTrain: 100% 2000/2000 [01:39<00:00, 20.11 step/s, accuracy=0.84, loss=3.76, step=56000]\nValid: 100% 5664/5667 [00:04<00:00, 1327.98 uttr/s, accuracy=0.85, loss=1.63]\nTrain: 100% 2000/2000 [01:39<00:00, 20.04 step/s, accuracy=0.94, loss=1.86, step=58000]\nValid: 100% 5664/5667 [00:04<00:00, 1282.51 uttr/s, accuracy=0.84, loss=1.61]\nTrain: 100% 2000/2000 [01:40<00:00, 19.87 step/s, accuracy=1.00, loss=0.12, step=6e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1285.82 uttr/s, accuracy=0.86, loss=1.56]\nTrain:   0% 4/2000 [00:00<01:48, 18.47 step/s, accuracy=0.97, loss=1.46, step=6e+4]","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.8565)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:41<00:00, 19.74 step/s, accuracy=0.91, loss=2.30, step=62000]\nValid: 100% 5664/5667 [00:04<00:00, 1282.83 uttr/s, accuracy=0.85, loss=1.60]\nTrain: 100% 2000/2000 [01:39<00:00, 20.20 step/s, accuracy=0.94, loss=1.86, step=64000]\nValid: 100% 5664/5667 [00:04<00:00, 1332.59 uttr/s, accuracy=0.86, loss=1.53]\nTrain: 100% 2000/2000 [01:38<00:00, 20.22 step/s, accuracy=0.97, loss=1.45, step=66000]\nValid: 100% 5664/5667 [00:04<00:00, 1303.99 uttr/s, accuracy=0.86, loss=1.50]\nTrain: 100% 2000/2000 [01:38<00:00, 20.28 step/s, accuracy=0.94, loss=2.65, step=68000]\nValid: 100% 5664/5667 [00:04<00:00, 1280.97 uttr/s, accuracy=0.86, loss=1.47]\nTrain: 100% 2000/2000 [01:38<00:00, 20.26 step/s, accuracy=0.94, loss=2.42, step=7e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1163.84 uttr/s, accuracy=0.87, loss=1.40]\nTrain:   0% 4/2000 [00:00<01:58, 16.88 step/s, accuracy=0.91, loss=2.47, step=7e+4]","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.8702)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:39<00:00, 20.20 step/s, accuracy=0.97, loss=1.57, step=72000]\nValid: 100% 5664/5667 [00:04<00:00, 1285.98 uttr/s, accuracy=0.87, loss=1.38]\nTrain: 100% 2000/2000 [01:37<00:00, 20.56 step/s, accuracy=0.94, loss=1.22, step=74000]\nValid: 100% 5664/5667 [00:04<00:00, 1284.52 uttr/s, accuracy=0.87, loss=1.44]\nTrain: 100% 2000/2000 [01:37<00:00, 20.57 step/s, accuracy=1.00, loss=0.40, step=76000]\nValid: 100% 5664/5667 [00:04<00:00, 1332.86 uttr/s, accuracy=0.87, loss=1.39]\nTrain: 100% 2000/2000 [01:37<00:00, 20.52 step/s, accuracy=1.00, loss=0.47, step=78000]\nValid: 100% 5664/5667 [00:04<00:00, 1337.07 uttr/s, accuracy=0.88, loss=1.35]\nTrain: 100% 2000/2000 [01:39<00:00, 20.14 step/s, accuracy=1.00, loss=0.20, step=8e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1275.69 uttr/s, accuracy=0.88, loss=1.38]\nTrain:   0% 4/2000 [00:00<01:51, 17.92 step/s, accuracy=1.00, loss=1.00, step=8e+4]","output_type":"stream"},{"name":"stdout","text":"Step 80000, best model saved. (accuracy=0.8775)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:39<00:00, 20.06 step/s, accuracy=1.00, loss=0.08, step=82000]\nValid: 100% 5664/5667 [00:05<00:00, 1004.01 uttr/s, accuracy=0.88, loss=1.33]\nTrain: 100% 2000/2000 [01:39<00:00, 20.01 step/s, accuracy=1.00, loss=0.48, step=84000]\nValid: 100% 5664/5667 [00:04<00:00, 1269.77 uttr/s, accuracy=0.88, loss=1.31]\nTrain: 100% 2000/2000 [01:38<00:00, 20.36 step/s, accuracy=0.97, loss=0.90, step=86000]\nValid: 100% 5664/5667 [00:04<00:00, 1218.64 uttr/s, accuracy=0.89, loss=1.25]\nTrain: 100% 2000/2000 [01:41<00:00, 19.75 step/s, accuracy=1.00, loss=0.83, step=88000]\nValid: 100% 5664/5667 [00:04<00:00, 1271.65 uttr/s, accuracy=0.89, loss=1.28]\nTrain: 100% 2000/2000 [01:39<00:00, 20.00 step/s, accuracy=0.97, loss=0.73, step=9e+4] \nValid: 100% 5664/5667 [00:04<00:00, 1274.34 uttr/s, accuracy=0.88, loss=1.27]\nTrain:   0% 4/2000 [00:00<02:01, 16.48 step/s, accuracy=0.97, loss=0.79, step=9e+4]","output_type":"stream"},{"name":"stdout","text":"Step 90000, best model saved. (accuracy=0.8900)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:42<00:00, 19.54 step/s, accuracy=0.97, loss=0.83, step=92000]\nValid: 100% 5664/5667 [00:04<00:00, 1314.50 uttr/s, accuracy=0.88, loss=1.29]\nTrain: 100% 2000/2000 [01:40<00:00, 19.96 step/s, accuracy=0.97, loss=1.16, step=94000]\nValid: 100% 5664/5667 [00:04<00:00, 1266.44 uttr/s, accuracy=0.89, loss=1.26]\nTrain: 100% 2000/2000 [01:39<00:00, 20.19 step/s, accuracy=1.00, loss=0.01, step=96000]\nValid: 100% 5664/5667 [00:04<00:00, 1297.57 uttr/s, accuracy=0.89, loss=1.26]\nTrain: 100% 2000/2000 [01:38<00:00, 20.24 step/s, accuracy=0.91, loss=2.67, step=98000]\nValid: 100% 5664/5667 [00:04<00:00, 1303.43 uttr/s, accuracy=0.88, loss=1.29]\nTrain: 100% 2000/2000 [01:38<00:00, 20.24 step/s, accuracy=1.00, loss=0.37, step=1e+5] \nValid: 100% 5664/5667 [00:04<00:00, 1260.82 uttr/s, accuracy=0.89, loss=1.22]\nTrain:   0% 0/2000 [00:00<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"Step 100000, best model saved. (accuracy=0.8928)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset of inference","metadata":{"id":"NLatBYAhNNMx"}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\n\n\nclass InferenceDataset(Dataset):\n    def __init__(self, data_dir):\n        testdata_path = Path(data_dir) / \"testdata.json\"\n        metadata = json.load(testdata_path.open())\n        self.data_dir = data_dir\n        self.data = metadata[\"utterances\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        utterance = self.data[index]\n        feat_path = utterance[\"feature_path\"]\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n\n        return feat_path, mel\n\n\n    def inference_collate_batch(batch):\n        \"\"\"Collate a batch of data.\"\"\"\n        feat_paths, mels = zip(*batch)\n\n        return feat_paths, torch.stack(mels)","metadata":{"id":"efS4pCmAJXJH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main funcrion of Inference","metadata":{"id":"tl0WnYwxNK_S"}},{"cell_type":"code","source":"import json\nimport csv\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"../input/cs4650dataset/Dataset\",\n        \"model_path\": \"./model.ckpt\",\n        \"output_path\": \"./output.csv\",\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    model_path,\n    output_path,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    mapping_path = Path(data_dir) / \"mapping.json\"\n    mapping = json.load(mapping_path.open())\n\n    dataset = InferenceDataset(data_dir)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=1,\n        shuffle=False,\n        drop_last=False,\n        num_workers=8,\n        collate_fn=InferenceDataset.inference_collate_batch,\n    )\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    speaker_num = len(mapping[\"id2speaker\"])\n    model = ConformerClassifier(n_spks=speaker_num).to(device)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    results = [[\"Id\", \"Category\"]]\n    for feat_paths, mels in tqdm(dataloader):\n        with torch.no_grad():\n            mels = mels.to(device)\n            outs = model(mels)\n            preds = outs.argmax(1).cpu().numpy()\n            for feat_path, pred in zip(feat_paths, preds):\n                results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n\n    with open(output_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerows(results)\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"id":"i8SAbuXEJb2A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}